%\documentclass[10pt, a4paper]{article}
\documentclass[submit]{smj}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, bbm}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage{apalike}
\usepackage{relsize}
\usepackage{array}
\usepackage{multirow}
%\usepackage{showlabels}
\usepackage{setspace}
\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{soul}
%Add line numbering
%Line numbering can be incorporated by using the lineno package. Add these statements in the preamble:
%\usepackage{lineno}
%\linenumbers

%\usepackage[nomarkers,figuresonly]{endfloat}

\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{prob}{Problem}
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
%%%%% bold symbol in math enviornment
\newcommand{\m}[1]{\boldsymbol{#1}}

\newcommand{\fmm}{\textsc{fmm}\xspace}
\newcommand{\X}{\text{\textbf{X}}}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} 

\Title{Merging the components of a finite mixture using  posterior probabilities}
\TitleRunning{Merging the components of an \fmm}
\Author{Marc Comas-Cufí\Affil{1}, Josep A. Martín-Fernández\Affil{1}, Glòria Mateu-Figueras\Affil{1}}
\AuthorRunning{Marc Comas-Cufí \textrm{et al.}}

\Affiliations{
\item Department of Computer Science, Applied Mathematics and Statistics, 
      Polytechnic School, 
      University of Girona,
      Spain
}

\CorrAddress{Josep A. Martín-Fernández, Department of Computer Science, Applied Mathematics and Statistics,
      University of Girona, P-IV, Campus Montilivi, E-17071 Girona, Spain}
\CorrEmail{josepantoni.martin@udg.edu}
\CorrPhone{(+34)\;616\;917\;016}
\CorrFax{(+34)\;972\;418\;792}

\Abstract{
Methods in parametric cluster analysis commonly assume data can be modelled by means of a finite mixture of distributions. However, associating each mixture component to one cluster is frequently misleading because different mixture components can overlap forming a unique cluster. A number of approaches have already been proposed to construct the clusters by merging components using the posterior probabilities. This paper presents a generic approach to building a hierarchy of mixture components that integrates and generalises some techniques proposed earlier in the literature. Using this proposal, two new techniques based on the log-ratio of posterior probabilities are proffered. Moreover, to decide the final number of clusters, two new methods are introduced. Simulated and real datasets are used to illustrate this methodology.
}

\Keywords{
Hierarchical clustering; logratio; merging components;  mixture model; model-based clustering; simplex
}
\begin{document}

%\begin{spacing}{1.9}
%%%%%%% BEGIN SPACING


\pagenumbering{arabic}

\maketitle

\section{Introduction}


% Mixture models
A common approach in parametric cluster analysis assumes data can be modelled by means of a \emph{finite mixture of distributions} \citep{fraley2002model, punzo2014flexible}, also called a \emph{finite mixture model} (\fmm). An \fmm is a probability distribution whose probability density function (pdf) is a linear combination of different distributions with same domain $\mathbb{X}$. More precisely, the pdf $f$ of an \fmm is
\begin{equation}\label{mixt}
f(\;\cdot\; ; \pi_1, \dots, \pi_k, \m\Theta) = \pi_1 f_1(\;\cdot\; ; \m\theta_1) + \dots + \pi_k f_k(\;\cdot\; ; \m\theta_k),
\end{equation}
where $\m\Theta =\{ \m\theta_1, \dots, \m\theta_k\}$, $\m\theta_j$ are the parameters of pdf $f_j$, $1\leq j \leq k$, and $\pi_j$ is the ``weight'' of component $f_j$. Restriction $\sum_{\ell = 1}^k \pi_\ell = 1$ guarantees that  $\int_{\mathbb{X}}f = 1$. Originally, the clustering algorithm based on an \fmm follows two steps:
\begin{enumerate}
\item to calculate estimates $\hat{\pi}_1, \dots, \hat{\pi}_k$ and $\hat{\m\Theta}$ of parameters $\pi_1, \dots, \pi_k$ and $\m\Theta$ using a sample $\X=\{\m x_1, \dots, \m x_n\}$ and
\item to classify each observation $\m x_i \in \X$ to a cluster c, $1\leq c \leq k$, according to the criterium of maximising the posterior probability
\[
\tau_{ij}= \frac{ \hat{\pi}_j f_j(\m x_i ; \hat{\m\theta}_j) }{\sum_{\ell=1}^k \hat{\pi}_\ell f_\ell(\m x_i ; \hat{\m\theta}_\ell) },
\]
that is, one observation $\m x_i$ is classified to cluster $c$ if
\begin{equation}\label{map_criteria}
c = \argmax_{j=1}^k \tau_{ij}.
\end{equation}
\end{enumerate}
Note that, in this process the number of clusters is equal to the number of mixture components. The different approaches to decide the number of components $k$ are reviewed in \cite{mclachlan2014components}. In our work we use the Bayesian Information Criterion (BIC) because under certain regularity conditions it estimates consistently the number of mixture components \citep{keribin1998consistent, keribin2000consistent}. In addition BIC is effective as a model selection criteria on a practical level \citep{fraley1998how}.

\cite{lee2004combining}, \cite{hennig2010methods}, \cite{baudry2010combining}, \cite{melnykov2013distribution} and \cite{pastore2013merging}, on the other hand, propose separating the concepts of cluster and mixture component. The authors show that associating each mixture component to one cluster can be misleading because different mixture components are frequently so overlapped that they can in fact be considered a unique cluster. In other words, one cluster could be formed as the result of merging two or more different mixture components. According to this approach, the \fmm clustering algorithm is completed with the following third step:

\begin{itemize}
\item[3.] to analyse which of the $k$ mixture components should be merged to form $k'$ clusters, $k' \leq k$.
\end{itemize}

The crux of this new step is how to decide which components have to be merged. Importantly, the underlying finite mixture does not change after two components have been merged. Therefore, for a given sample the likelihood function remains invariant. This fact makes it impossible to decide which components form a single cluster in terms of the likelihood or through the BIC criteria \citep{hennig2010methods}. Our approach is based on the posterior probabilities which are obtained after adjusting an \fmm. We introduce a generic expression that integrates and generalises methods given by \cite{baudry2010combining}, \cite{hennig2010methods} and \cite{longford2014}. In addition, with this new approach the analyst can define their own method, for example, based on log-ratio transformations of posterior probabilities \citep{aitchison1986statistical}. 
Using this generic approach for merging components one can also build a hierarchy over the set of mixture components. On the first level of this hierarchy we consider $k$ clusters where each cluster is modelled by one component, and on the second level we have $k-1$ clusters where one cluster is modelled by two components. On the successive levels subsequent clusters are modelled by merging different components. The final level contains one cluster (the original sample) modelled by a single component (the original mixture).

The paper is organised as follows. Section~\ref{definitions} introduces the definitions and notation required throughout this paper and Section~\ref{generic_merging} presents the general merging criteria and shows that the most important techniques from literature can be condensed into this new approach. Using this proposal, Section~\ref{logratio_section} presents a new family of techniques based on the log-ratio methodology. In Section~\ref{number_clusters}, two heuristic methods to decide the final number of clusters are proposed and in Section~\ref{merging_examples_dist} two examples to illustrate the algorithm with different types of mixture distributions are presented. To conclude, final remarks are made in Section~\ref{remarks}. The programming of the data analyses discussed in this work has been conducted using the open-source R statistical environment \citep{R}. Computer routines implementing the methods can be obtained from the website {\em http://www.compositionaldata.com} and also from the R package ``zCompositions".

\section{Definitions and notation}\label{definitions}

%
Let $\mathcal{I}^k = \{1, \dots, k\}$ be a set of natural numbers to indicate the components of an \fmm. A \emph{partition} of $\mathcal{I}^k$ of size $s$, $\mathcal{P}_s$, is the disjoint union of subsets $I_p$, called \emph{parts}. More formally, $\mathcal{P}_s$ is a set with $s$ subsets $I_p$  that $\bigcup_{I_p \in \mathcal{P}_s} I_p = \mathcal{I}^k$ and for any two parts $I_a, I_b \in \mathcal{P}_s$ with $a \neq b$, $I_a \cap I_b = \emptyset$ holds. Given a partition  $\mathcal{P}_s$, the pdf $f$ of an \fmm (Equation~\ref{mixt}) can be written as
\begin{equation}
f = \pi_{I_1} f_{I_1}(\;\cdot\;; \m\Theta) + \dots + \pi_{I_s} f_{I_s}(\;\cdot\;; \m\Theta),
\label{mixt_part}
\end{equation}
where $f_{I_p}(\;\cdot\;;  \m\Theta) = \sum_{j \in I_p} \frac{\pi_j}{\pi_{I_p}} f_j(\;\cdot\; ; \m\theta_j)$ and $\pi_{I_p} = \sum_{\ell \in I_p} \pi_\ell$. Note that using this notation each $f_{I_p}(\;\cdot\;;  \m\Theta)$ is also an \fmm. Because each part $I_p$ defines a single component $f_{I_p}$, when there is no confusion, we use $I_p$ referring either to the part $I_p$ or to the component $f_{I_p}$. Note that, given $k$ components of an \fmm $f$, there is $B_k$ different ways to express the mixture  $f$ in terms of a partition.\footnote{$B_k$ is the $k$-th Bell number defined recursively as $B_0 = 1$ and $B_k = \sum_{i=0}^{k-1} \binom ki B_{i-1}$.}


A \emph{hierarchical sequence of partitions of $\mathcal{I}^k$} is a sequence $\mathcal{P}_1, \dots, \mathcal{P}_k$ verifying that
\begin{itemize}
\item $\mathcal{P}_1$ is the one-part partition $\mathcal{P}_1 = \{ \mathcal{I}^k \}$,
\item if a part $I_p \in \mathcal{P}_{s-1}$ then either there is a part $I_a \in \mathcal{P}_{s}$ with $I_p = I_a$ or there are two parts $I_a, I_b \in \mathcal{P}_s$ with $I_p = I_a \cup I_b$, and
\item $\mathcal{P}_k= \{ \{1\},\{2\}, \dots, \{k\} \}$.
\end{itemize}



One can extend Equation~\ref{map_criteria} in terms of partitions. Indeed, let $\X = \{\m x_1,\dots, \m x_n\}$ be a sample formed by observations of $\mathbb{X}$. Given a partition $\mathcal{P}_s = \{ I_1, \dots, I_s \}$, we define the posterior probability of $\m x_i$ being classified to part $I_p\in \mathcal{P}_{s}$ as
\[
\tau_{i I_p} =  \frac{ \hat{\pi}_{I_p} f_{I_p}(\m x_i; \hat{\m\Theta}) }{\sum_{\ell=1}^s \hat{\pi}_{I_\ell} f_{I_\ell}(\m x_i; \hat{\m\Theta})}
\]
where $\hat{\pi}_{I_p} = \sum_{\ell \in I_p} \hat{\pi}_\ell$. Then, the posterior probability vector associated to observation $\m x_i$ is
\begin{equation}\label{ppv}
\m\tau_{i \mathcal{P}_s} = \left(\tau_{i I_1} , \dots, \tau_{i I_s}  \right).
\end{equation}
The posterior probability vector $\m \tau_{i \mathcal{P}_s}$ denotes the conditional probability that $\m x_i$ arises from mixture components $f_{I_1}, \dots, f_{I_s}$. Since $\mathcal{P}_s$ is a partition $\sum_{p=1}^s \tau_{i I_p} = 1$ holds  for $1 \leq i \leq n$. Similarly to Equation~\ref{map_criteria}, the posterior probability vectors $\m\tau_{i \mathcal{P}_s}$ can be used to classify $\m x_i \in \X$ to the cluster $c$, $1\leq c\leq s$, if
\begin{equation}\label{cluster_criteria}
c= \argmax_{p=1}^s \{ \tau_{i I_p} \}.
\end{equation}

Let $\text{\textbf{T}}_{\mathcal{P}_s}$ be the matrix with $n$ rows and $s$ columns formed by the $n$ vectors of posterior probabilities $\m \tau_{i\mathcal{P}_s}$ associated to partition $\mathcal{P}_s$. For example, $\text{\textbf{T}}_{\mathcal{P}_k}$ is the initial matrix, when the number of components and clusters are equal. Importantly, with Equation \ref{ppv}, any matrix $\text{\textbf{T}}_{\mathcal{P}_s}$ can be obtained from matrix $\text{\textbf{T}}_{\mathcal{P}_k}$ respectively aggregating the corresponding columns of each of the parts $I_1, \dots, I_s$.


\section{Generic approach}\label{generic_merging}

\subsection{General merging criteria}\label{merging_criteria}

Let $\text{\textbf{X}} = \{\m x_1,\dots,\m x_n \}$ be a sample defined in  a domain $\mathbb{X}$ and let $f$ be an \fmm with $k$ components defined on $\mathbb{X}$ (Equation~\ref{mixt}). Given a partition $\mathcal{P}_s = \{I_1, \dots, I_s\}$, let $\m\tau_{i \mathcal{P}_s}= \left( \tau_{i I_1} , \dots, \tau_{i I_s}  \right)$ be the posterior probability vector associated to observation $\m x_i$, $1\leq i \leq n $.

For a partition $\mathcal{P}_s$  and matrix of posterior probabilities $\text{\textbf{T}}_{\mathcal{P}_s}$, we propose merging the parts $I_a, I_b \in \mathcal{P}_s$ maximising the weighted mean
\begin{equation}\label{unifying_equation}
S_{\omega, \lambda}( \text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) = \frac{\sum_{i=1}^n \omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b) \; \lambda(\m\tau_{i \mathcal{P}_s}, I_a, I_b)}{\sum_{i=1}^n \omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b) },
\end{equation}
where $\lambda(\m\tau_{i \mathcal{P}_s}, I_a, I_b)$ is a real valued function and $\omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b)$ is a non-negative function. We refer to this maximum as the $S$-value. Function $\lambda(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$ has the role of a utility function. It measures our preferences for considering components $f_{I_a}$ and $f_{I_b}$ as a single component, and therefore, to model it by means of the corresponding merged component. Function $\omega(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$ is a weight function. It permits the influence that each posterior probability vector $\m\tau_{i \mathcal{P}_s}$ has in Equation~\ref{unifying_equation} for parts $I_a$ and $I_b$ to be modified. The behaviour of function $S_{\omega, \lambda}$ is wholly etermined by the choice of functions $\omega$ and $\lambda$. Importantly, function $S_{\omega, \lambda}$ has the same codomain as function $\lambda$ has.

Starting from partition $\mathcal{P}_k = \{ \{1\}, \dots, \{k\} \}$, where the number of clusters is equal to the number of components, two of these parts are merged according to the $S$-value (Equation~\ref{unifying_equation}). Iteratively repeating this process, the algorithm builds an agglomerative hierarchical sequence of partitions until partition $\mathcal{P}_1$ is obtained. Remarkably, by the definition of functions $\omega$ and $\lambda$, the process only depends on the posterior probability vectors $\text{\textbf{T}}_{\mathcal{P}_s}$.

For the initial partition $\mathcal{P}_k$, we need to evaluate $k^2-k$ times function $S_{\omega, \lambda}$ to obtain  the corresponding $S$-value. The process is repeated from partition $\mathcal{P}_{k-1}$ to $\mathcal{P}_1$, evaluating the function $S_{\omega, \lambda}$ a maximum of $\frac{k^3-k}{3}$ times. These quantities are reduced by half when the function $S_{\omega, \lambda}$  is symmetric with regards to $I_a$ and $I_b$. 

\subsection{Minimising the final entropy}
\label{entropy_section}

\cite{baudry2010combining} propose an algorithm to build a hierarchical sequence of partitions based on the concept of entropy. The Shannon entropy of a posterior probability vector $\m\tau_{i \mathcal{P}_s} = \left( \tau_{i I_1} , \dots, \tau_{i I_s}  \right)$ is
\[
\text{Ent}( \m\tau_{i \mathcal{P}_s} ) = -\sum_{j=1}^s \tau_{i I_j}  \log(\tau_{i I_j} ).
\]

The entropy can be interpreted as a measure of similarity between a probability vector $\m\tau_{i \mathcal{P}_s}$ and the probability vector $\m\tau^0_{s}=\left(\frac{1}{s}, \dots, \frac{1}{s}\right)$, taking the maximum value $-\log(\frac{1}{s})$ when $\m\tau_{i \mathcal{P}_s}=\m\tau^0_{s}$. Given a partition $\mathcal{P}_s = \{ I_1, \dots, I_s\}$ the algorithm iteratively merges  the two mixture components optimising the overall entropy. Let $\mathcal{P}_{s-1}^{I_a\cup I_b}$ be the partition obtained after merging components $I_a$ and $I_b$ from $\mathcal{P}_s$. The parts $I_a$ and $I_b$ merged minimise expression
\[
\sum_{i=1}^n \text{Ent}( \m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}} ).
\]

According to \cite{baudry2010combining}, minimising the previous expression is equivalent to maximising the loss of entropy, that is, maximising the sum
\[
\sum_{i=1}^n  \left\{ \text{Ent}( \m\tau_{i \mathcal{P}_s} ) - \text{Ent}( \m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}} ) \right\}.
\]
which can be written only in terms of $\tau_{i I_a}$  and $\tau_{i I_b}$ as
\begin{equation}\label{entropy}
\sum_{i=1}^n   \left\{(\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \left[\tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b})\right] \right\}.
\end{equation}
Note that using our general merging criteria we can define function $\lambda_{\,\Delta\text{Ent}}$ as
\[
\lambda_{\,\Delta\text{Ent}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) :=  (\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \left[ \tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b}) \right],
\]
and function $\omega_{\text{cnst}}$ as a constant, for example 
\[
\omega_{\text{cnst}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := 1.
\]

When assuming that $\omega_{\text{cnst}}$ is constant, we are considering each observation as being equally important to compute the $S$-value. That is, the weighting is the same regardless of the parts $I_a$ and $I_b$ we are willing to merge. In this case, the $S$-values (Equation~\ref{unifying_equation}) take the form
\[
\begin{split}
S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}( \text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) = \frac{1}{n} \sum_{i=1}^n & \left\{(\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \right.\\ 
&\quad \left.\left[ \tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b}) \right]\right\},
\end{split}
\]
i.e. the average of loss of entropy.

Note that in this approach function $S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$ is symmetric with respect to $I_a$ and $I_b$. Therefore, for partition $\mathcal{P}_k$ we only need to evaluate function $S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$ for $\frac{k^2-k}{2}$ times. Note also that for partition $\mathcal{P}_{k-1}$ we only need to update the value of $S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$ for $k-2$ different values. Finally, to calculate the $S$-values in the whole process we only need to evaluate the function $S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$ $(k-1)^2$ times. 

\subsection{Maximising the misclassification probability}
\label{missclassification_section}

\cite{hennig2010methods} proposes merging the components $I_a$ and $I_b$ from $ \mathcal{P}_s$ that maximise \emph{the probability of classifying to component $I_b$ an observation generated from component $f_{I_a}$}. To estimate this probability,  \cite{hennig2010methods} introduces a consistent estimator, the Directed Estimated Misclassification Probabilities (DEMP), defined as
\[
\frac{ \frac{1}{n} \sum_{i=1}^n {\tau_{iI_a} \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)}}{ \hat{\pi}_{I_a}},
\]
where $\mathbbm{1}\left( \cdot \right)$ is the indicator function. Because $ \hat{\pi}_{I_a} = \frac{1}{n} \sum_{i=1}^n \tau_{iI_a}$, the estimator DEMP can be written in terms of posterior probability as
\begin{equation}\label{demp_criteria}
\frac{ \sum_{i=1}^n {\tau_{iI_a} \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)}}{\sum_{i=1}^n \tau_{iI_a} }.
\end{equation}

Note that when parts $I_a$ and $I_b$ overlap, Equation~\ref{demp_criteria} takes higher values because the posterior probability $\tau_{i I_b}$ is higher for observations $i$ generated by component $f_{I_a}$.

Using our general merging criteria, the estimator DEMP (Equation~\ref{demp_criteria}) is equivalent to set functions $\omega$ and $\lambda$ as 
\begin{equation}\label{lambda_demp}
\lambda_{\text{DEMP}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)
\end{equation}
and
\[
\omega_{\text{prop}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) :=  \tau_{iI_a}.
\]

In this approach function $\lambda_{\text{DEMP}}$ is giving preference to observations $\m x_i$ classified to part $I_b$. Moreover, the $\omega_{\text{prop}}$ function is weighing higher those observations with high posterior probability $\tau_{iI_a}$. This approach permits us to calculate the $S$-value from $I_a$ to $I_b$ measuring our preference to merge $I_a$ into $I_b$. Note that in this case function  $S_{\omega_{\text{prop}}, \lambda_\text{DEMP}}$ is not symmetric, i.e. $S_{\omega_{\text{prop}}, \lambda_\text{DEMP}}(\text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) \neq S_{\omega_{\text{prop}}, \lambda_\text{DEMP}}(\text{\textbf{T}}_{\mathcal{P}_s},  I_b,  I_a)$. Therefore, to calculate the $S$-values we need to evaluate the function $S_{\omega_{\text{prop}}, \lambda_\text{DEMP}}$ $2 (k-1)^2$ times. In addition, this lack of symmetry indicates that the merging process is extended to a type of absorbation process of one component by the other.

\cite{longford2014} propose a variation of the approach introduced in \cite{hennig2010methods}. Rather than considering function $\lambda_{\text{DEMP}}$ given by Equation~\ref{lambda_demp}, in this case the preference for merging $I_a$ into $I_b$ is given by
\begin{equation}\label{lambda_dempM}
\lambda_{\text{DEMP}_m}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \frac{\tau_{iI_b}}{\tau_{iI_a} + \tau_{iI_b}}.
\end{equation} 


When a method uses function $\lambda_{\text{DEMP}_m}$ we call it a DEMP modified approach. Similarly to function $\lambda_{\text{DEMP}}$, function $\lambda_{\text{DEMP}_m}$ gives high scores to observations with high posterior probability $\tau_{iI_b}$. In this case, $\lambda_{\text{DEMP}_m}$ is the probability of being generated by component $f_{I_b}$ conditioned being generated by either $f_{I_a}$ or $f_{I_b}$. Again, function $\lambda_{\text{DEMP}_m}$ causes function $S_{\omega, \lambda_{\text{DEMP}_m}}$ not to be symmetric. The function $\lambda_{\text{DEMP}_m}$, having the codomain $[0,1]$, increases when the probability $\tau_{iI_b}$ increases, and decreases when the probability $\tau_{iI_a}$ is increased. 


\section{Other criteria: the logratio approach}\label{logratio_section}


The generic expression of $S_{\omega, \lambda}(\text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b)$ (Equation~\ref{unifying_equation}) allows us to extend the criteria for merging components to expressions defined by the analyst. For example, in \cite{hennig2010methods} and \cite{longford2014} observations with high $\tau_{I_b}$ are preferred. In this context, it might be reasonable to consider
\[
\lambda_{\text{prop}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \tau_{iI_b}.
\]
In this case, function $\lambda_{\text{prop}}$ combined with $\omega_{\text{prop}}$ results in an algorithm with an easily computable function $S_{\omega_\text{prop}, \lambda_\text{prop}}$. As an alternative to function $\omega_{\text{prop}}$ we can define 
\[
\omega_{\text{dich}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \mathbbm{1}\left( \forall j\; \tau_{i I_{a}} \geq \tau_{iI_j} \right).
\]
This function gives weight one to observations classified to part $I_a$ and zero to the others, which, in turn, results in an average of $\lambda$ values for observations classified to part $I_a$. 

Although there are many possible functions $\omega$ and $\lambda$, we present two alternatives to function $\lambda$ from a well-founded background.. \cite{aitchison1986statistical} introduces the main elements for the statistical analysis of samples defined in the Simplex space $\mathcal{S}^d$, i.e $\mathcal{S}^d = \{ (x_1,\dots, x_d) \;|\; x_i > 0 \text{ and } \sum_{i=1}^k x_i = 1 \}$. The central idea of this methodology is that only the ratios between variables are of interest. Here we take advantage that posterior probability vectors $\m\tau_{i\mathcal{P}_s}$ are defined in $\mathcal{S}^s$ to define two new functions $\lambda$. The first function is motivated by the Entropy concept introduced by \cite{baudry2010combining}, while the second one is based on the definition given by \cite{longford2014}.


Following \cite{baudry2010combining}, the closer the posterior probability vector $\m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}}$ is to the vector of the uniform distribution $\m\tau_s^0$ is, the higher our preference to merge components $I_a$ and $I_b$ (the lower is the entropy after merging $I_a$ and $I_b$). To measure our preference for merging $I_a$ and $I_b$ we propose calculating the similarity between the vector $\m\tau_{\left(I_a, I_b\right)} = (\frac{\tau_{i I_a}}{\tau_{i I_a} + \tau_{i I_b}}, \frac{\tau_{i I_b}}{\tau_{i I_a} + \tau_{i I_b}})$ and the vector $\m\tau_2^0=\left(\frac{1}{2}, \frac{1}{2}\right)$. That is, we restrict only on the probabilities associated to the components to be merged. According to \cite{frey2007}, we can use an appropriate distance to define the similarity measure. Indeed, using the squared Aitchison distance \citep{palarea2012dealing} given by
\[
d_\mathcal{A}^2\left(\m\tau_{\left(I_a, I_b\right)}, \m\tau_2^0 \right) = \log^2 \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right).
\]

we can define the similarity function
\[
\lambda_{\text{dist}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := -d^2_\mathcal{A}\left(\m\tau_{\left(I_a, I_b\right)}, \m\tau_2^0 \right) = -\log^2 \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right).
\]



Similarly to function $\lambda_{\Delta\text{Ent}}$, the function $\lambda_{\text{dist}}$  measures how close $\m\tau_{\left(I_a, I_b\right)}$ is to $\m\tau_2^0$. However, the codomain of function $\lambda_{\Delta\text{Ent}}$ also implicitely depends on the posterior probabilities for the other parts different of those to be merged. Figure~\ref{symetric} shows these functions for a partition with three components $I_a$, $I_b$, and $I_c$. For different values of $\tau_{iI_c}$, the curves represent the effect of $\tau_{I_a}$ and $\tau_{I_b}$ into functions $\lambda$. We see that both functions $\lambda$ take their maximum for $\tau_{iI_a}=\tau_{iI_b}=\frac{1-\tau_{i I_c}}{2}$. However, whereas the maximum value of function $\lambda_{\text{dist}}$ is zero regardless the value of $\tau_{iI_c}$ (Figure~\ref{symetric}(right)), the codomain of function $\lambda_{\Delta\text{Ent}}$ depends on $\tau_{iI_c}$ (Figure~\ref{symetric}(left)). This fact suggests that the selection of the best components to be merged using $\lambda_{\Delta\text{Ent}}$ based on the corresponding $S$-value can be affected by the size of the clusters. This effect agrees with the performance described by \cite{baudry2010combining} when the author apply their own criteria.

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.8\textwidth]{figures/entr_dist.pdf} \\
 \end{tabular}
 \caption{Function $\lambda$ for posterior probability 
vector $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c} \right)$ with $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$: (left) $\lambda_{\text{Ent}}$; (right) $\lambda_{dist}$.} 
\label{symetric}
\end{center}
\end{figure}

Following the function $\lambda_{\text{DEMP}_m}$ introduced by \cite{longford2014} (Equation \ref{lambda_dempM}), we can define another function $\lambda$ using log-ratios. We propose  measuring the relative difference between $\tau_{iI_b}$ an $\tau_{iI_a}$ with the log-ratio
\[
\lambda_{\log}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \log \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right),
\]

which increases when the probability $\tau_{iI_b}$ increases, and decreases when the probability $\tau_{iI_a}$ increases.

Figure~\ref{nonsymetric} shows the behaviour of functions $\lambda_{\text{DEMP}_m}$ and $\lambda_{\log}$ for posterior probability vectors $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c}\right)$ when $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$. Note that the maximum value of both functions is not affected by the value $\tau_{iI_c}$. However, whereas the codomain of  function $\lambda_{\text{DEMP}_m}$ is the interval $[0,1]$, the codomain of function $\lambda_{\log}$ is the Real space. We can see that the highest values of function $\lambda_{\log}$ occur for observations with high $\tau_{iI_b}$ relative to $\tau_{iI_a}$. Because the comparison is relative, observations not related to parts $I_a$ or $I_b$ are able to play an important role in the final $S$-value. Therefore, the selection of function $\omega(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$ is especially important. To weight higher those observations related to part $I_a$, a reasonable selection for function $\omega$ would be $\omega_{\text{prop}}$ or $\omega_{\text{dich}}$. In this context, because we need to weight higher those observations related with part $I_a$, function $\omega_{\text{cnst}}$ makes no sense.

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.8\textwidth]{figures/demp2_log.pdf} \\
 \end{tabular}
 \caption{Function $\lambda$ for posterior probability vector $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c} \right)$ with $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$: (left) $\lambda_{\text{DEMP}_m}$; (right) $\lambda_{\log}$.} 
\label{nonsymetric}
\end{center}
\end{figure}

As $\lambda_{\text{dist}}$ and $\lambda_{\log}$ use logratios they take into an account the geometric properties of the Simplex space \citep{aitchison2002simplicial}.  When working with log-ratios between the components of a posterior probability vector,  \emph{subcompositional coherence} holds  \citep{aitchison1986statistical}. This property guarantees that any statistical inference obtained using only partial information is coherent with results obtained using complete information. Formally, in our context sub-compositional coherence can be defined as:

\begin{defn}
Let $\mathcal{P}_1, \dots, \mathcal{P}_k$ be a hierarchical sequence of partitions obtained from posterior probability matrix $\text{\textbf{T}}_{\mathcal{P}_k}$ using a merging approach $M$. Let $I = \{j_1, \dots, j_s\}$ be an element (a part) of $\mathcal{P}_s$, for some $s$, $1\leq s \leq k$. A method M is \emph{sub-compositional coherent} if the hierarchy subsequence of partitions obtained  using only posterior probability vectors $\left\{ \left(\tau_{ij_1}, \dots, \tau_{ij_s} \right)\right\}_{1\leq i \leq n}$ is contained in the original hierarchy.
\end{defn}

Another interesting feature of the log-ratio approach is the \emph{scale invariance} property \citep{aitchison1986statistical}, formally
\[
S_{\omega, \lambda}( \m\tau_{\mathcal{P}_s},  I_a,  I_b) = S_{\omega, \lambda}(k\cdot \m\tau_{\mathcal{P}_s},  I_a,  I_b) \text{ for $k>0$.}
\] 

This property suggests that the log-ratio approach is not restricted only to posterior probability vectors. On the contrary, it can be applied to any other kind of vector giving relative information between mixture components. That is, the two methods considered here are suitable to be applied in more general scenarios such as using vectors of weights of parts (e.g. weights in fuzzy clustering).

Remarkabely, when $\omega_{\text{prop}}$ and $\lambda_{\log}$ are considered, the $S$-value (Equation~\ref{unifying_equation}) results in
\[
S_{\omega_{\text{prop}}, \lambda_{\log}}( \m\tau_{\mathcal{P}_s},  I_a,  I_b) = \frac{\sum_{i=1}^n \tau_{iI_a}  \log \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right)}{\sum_{i=1}^n \tau_{iI_a}}.
\]
For a fixed component $I_a$, the denominator is constant and we only need to maximise the numerator. The expression in the numerator has the essence of the Kullback-Leibler divergence (in negative sign) comparing the distribution of classifying observations to $I_a$ against the distributions of classifying the same observations to $I_b$.

\section{Deciding the number of clusters}\label{number_clusters}

Given a finite mixture adjusted with BIC criteria, we have presented a generic approach to build a hierarchical sequence of partitions. One of the main difficulties is deciding the final number of clusters. For any partition $\{I_1, \dots, I_{s}\}$ the likelihood function of mixture $f = \pi_{I_1} f_{I_1} + \dots + \pi_{I_s} f_{I_s}$ is always the same. In other words, from a frequentist perspective, it is not possible to decide which of the different ways of modelling a cluster with different components is the best  \citep{hennig2010methods}. Therefore, we need to use heuristic methods to decide the final number of clusters.

\subsection{Using $S$-values}

A first option when deciding the number of clusters is the $S$-values. In the case of $\omega_{\text{cnst}}$ and $\lambda_{\Delta\text{Ent}}$, \cite{baudry2010combining} propose  visualising the $S$-values and apply the elbow rule. For $\omega_{\text{prop}}$ and $\lambda_{\text{DEMP}}$, \cite{hennig2010methods} propose setting an arbitrary threshold and stop merging when the $S$-value is lower than the fixed threshold. Although there is no rule of thumb to define a method for the general merging criteria, for the particular proposals described in Section \ref{generic_merging} and \ref{logratio_section}, the $S$-values can be a useful tool in deciding the number of clusters. Indeed, from its definition as maximum weighted average of $\lambda$ values, a very \emph{small} $S$-value might suggest stopping the merging process.

To better interpret the $S$-values we can normalise them to the interval $\left[0,1\right]$ using a monotone function. Two options are feasible: scaling function $\lambda$ or scaling function $S_{\omega, \lambda}$. Note that any function $\phi$ that scales function $\lambda$ also scales function $S_{\omega, \lambda}$. We denote these two scaling options by $S_{\omega, \phi \circ \lambda}$ and $\phi \circ S_{\omega, \lambda}$. Importantly, the first approach $S_{\omega, \phi \circ \lambda}$, is modifying function $\lambda$, and therefore, it can be considered as a new method in itself. It is worth mentioning that for $\phi_{\log}(x)=\frac{e^x}{1+e^x}$, the scaling $S_{\omega, \phi_{\log} \circ \lambda_{\log}}$ reduces to $S_{\omega, \lambda_{\text{DEMP}_m}}$. That is, the  function $\lambda_{\text{DEMP}_m}$ \citep{longford2014} is a normalised version of the function $\lambda_{\log}$.

From their definition the functions $\lambda_{\Delta\text{Entr}}$, $\lambda_{\text{dist}}$ and 
$\lambda_{\log}$ are not scaled into the interval $\left[0,1\right]$. For scaling these functions we propose  $\phi_{\Delta\text{Entr}}(x) = -x/{\log(\frac{1}{s})}$, $\phi_{\text{dist}}(x) = 1-e^{-x}$ and $\phi_{\log}(x) = \frac{e^x}{1+e^x}$, respectively. Despite these functions not being the only possibilities for the scaling, we selected them because of their reasonable performance in our experiments. The corresponding scaled $S$-value functions are: 
  $\phi_{\Delta\text{Entr}} \circ S_{\omega, \lambda_{\Delta\text{Entr}}}$ or $S_{\omega, \phi_{\Delta\text{Entr}} \circ \lambda_{\Delta\text{Entr}}}$, $\phi_{\text{dist}} \circ S_{\omega, \lambda_{\text{dist}}}$ or $S_{\omega, \phi_{\text{dist}} \circ \lambda_{\text{dist}}}$, and $\phi_{\log} \circ S_{\omega, \lambda_{\log}}$ or $S_{\omega, \phi_{\log} \circ \lambda_{\log}}$. Once the $S$-values are normalised, one reasonable rule of thumb is to stop the merging process when these values are close to zero.

\subsection{Using the location of the posterior probabilities}\label{coda_clusters}

For a sample with $s$ well-separated clusters of observations, when each cluster is modelled by a different probability distribution, the posterior probability vectors of the elements of each cluster are located close to a vertex of the Simplex $\mathcal{S}^s$. For example, in Figure~\ref{cluster_post} we have a sample generated following a mixture of four Gaussian distributions. In the first scenario (top-left) each component models one different cluster, while in the second scenario (bottom-left), we are modelling three clusters where the two components centred at $(10, 10 \sqrt{3})$ are modelling one cluster together. When we consider the sample with four clusters, the posterior probability vectors can be represented in a quaternary diagram (top-right). In the quaternary diagram we see the posterior probability vectors of parts $I_1$ and $I_3$ are respectively located close to vertices $\tau_1$ and $\tau_3$ of $\mathcal{S}^4$. In contrast, the posterior probability vectors of individuals $I_2$ and $I_4$ are not well separated and they expand throught the edge running from $\tau_2$ to $\tau_4$. The situation changes if we consider the three cluster sample (bottom-left). In this case, we can represent the posterior probability vectors in a ternary diagram (bottom-right) to discern that the posterior probability vectors of all three parts are located in three different vertices.

\begin{figure}[thbp]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
 % \includegraphics[width=0.7\textwidth]{figures/cluster_post.pdf} \\
\includegraphics[width=0.5\textwidth]{figures/ex_4clust.pdf}
\includegraphics[width=0.5\textwidth]{figures/ex_quaternaryb.pdf}\\
\includegraphics[width=0.5\textwidth]{figures/ex_3clust.pdf}
\includegraphics[width=0.5\textwidth]{figures/ex_ternary.pdf}
 \end{tabular}
 \caption{Top-Left: Sample generated from a mixture of four Gaussian distributions where each component is considered as one cluster. Top-Right: Posterior probability vectors of the sample represented in a quaternary diagram. Bottom-Left: Same sample where the four components are modeling three clusters. Bottom-Right: Posterior probability vectors of the sample represented in a ternary diagram.}\label{cluster_post}
\end{center}
\end{figure}

Therefore, we can use the geometric structure of $\mathcal{S}^s$ \citep{PE2001geo} to define the number of clusters based on how close the posterior probability vectors are to the vertices and how \emph{well} separated the posterior probability vectors associated to each cluster are.

Let $I_1,\dots, I_s$ be the parts defining the $s$ clusters. The posterior probability vectors, $\m\tau_i$, of observations $\m x_i$ assigned to $I_a$ are close to the corresponding vertex if $\tau_{i I_a}$ is close to one. In addition, $\tau_{i I_b}$, for $b \neq a$, are close to zero. Consequently, the log-ratio $\log(\tau_{i I_a}/\tau_{iI_b})$ should take larger values for all observations $\m x_i$ assigned to cluster $I_a$. To decide how close $\m\tau_i$ is to the vertex of the Simplex we calculate
\[
\nu_{i, a} = \min_{b,\;b \neq a}  \log(\frac{\tau_{i I_a}}{\tau_{iI_b}}).
\]
Because $\tau_{i I_a}$ is the highest posterior probability (observation $x_i$ is assigned to $I_a$), $\nu_{i a}$ is positive. Importantly, the lower the value $\nu_{i, a}$, the farther is $\m\tau_{i}$ from vertex $I_a$. For example, when considering four clusters (Figure \ref{cluster_post} (top-left)) the average of $\nu_{i, j}$, for observations assigned to clusters centred  at $(0,0)$ and $(20, 0)$ is $15.83$ and $14.46$, respectively. In contrast, the average of $\nu_{i, j}$ for observations assigned to clusters centred at $(10, 10\sqrt{3})$ are significatively lower ($2.15$ and $4.00$). When three clusters are considered (Figure \ref{cluster_post} (bottom-left)), the average of $\nu_{i, j}$ for observations assigned to clusters centred at at $(0,0)$, $(20, 0)$ and $(10, 10\sqrt{3})$ are 15.82, 14.46 and 19.48, respectively. 

Therefore, using this measure to identify if there are two clusters not \emph{well} separated we calculate the following index:
\[
\mathcal{V} = \min_{j \in \{1,\dots, s\}} \nu_{i, j}.
\]
In the first scenario (Figure \ref{cluster_post} (top-right)) $\mathcal{V} = 2.15$ and, in the second scenario (Figure \ref{cluster_post} (bottom-right)) $\mathcal{V} = 14.46$.


A different approach to identifying if the posterior probability $\m\tau_{i}$ are close to the vertices is to identify if the $\m\tau_{i}$ associated to parts are forming a cluster by themselves. For example, in Figure~\ref{cluster_post} (top-right) the posterior probability vectors associated to $\tau_2$ and $\tau_4$ are not forming two well-separated clusters. To measure this separation we propose using indices that can be calculated using distances between individuals. For this purpose, we used the Calinski-Harabasz (G1) and the Goodman and Kruskal (G2) indices \citep{milligan1985} using the Aitchison distance in $\mathcal{S}^s$. For example, the indices $G1$ and $G2$ respectively take the values $350.28$ and $0.68$ for four-clusters solution in the Figure~\ref{cluster_post} (top-right), whereas for three-clusters solution the corresponding values are $759.49$ and $0.85$ (bottom-right). This differences indicate that the structure with three clusters seems to be more adequate for the sample.

\section{Examples}\label{merging_examples_dist}

\subsection{Merging components in a mixture of Gaussian distributions}

Consider the bivariate Gaussian mixture of six components \citep{baudry2010combining}
\[
f= \sum_{j=1}^6 \pi_j \phi(\;\cdot\; ;  \m\mu_j, \m\Sigma_j)
\]
with the parameters shown in the Table~\ref{pars_table}. 

\begin{table}[htpb]
\caption{Parameters defining a two dimensional Gaussian mixture with six components. The parameters $\m\mu_j$ and $\m\Sigma_j$ are expressed in terms of the univariate means $\mu_{j x_1}$, $\mu_{j x_2}$, the univariate variances $\sigma^2_{j x_1}$, $\sigma^2_{j x_2}$ and the correlation between $x_1$ and $x_2$, $\rho_{j x_1 x_2}$.}
\centering
\begin{tabular}{rrrrrrr}
  \hline
$j$ & $\pi_j$ & $\mu_{j x_1}$ & $\mu_{j x_2}$ & $\sigma^2_{j x_1}$ & $\sigma^2_{j x_2}$ & $\rho_{j x_1 x_2}$ \\ 
  \hline
  1 &  $1/6$ &     0 &     0 &    50 &     5 &     0 \\ 
  2 &  $1/6$  &     0 &    40 &     5 &    50 &     0 \\ 
  3 &  $1/6$  &    40 &    40 &     5 &    50 &     0 \\ 
  4 &  $1/6$  &     0 &     0 &     5 &    50 &     0 \\ 
  5 &  $1/6$  &    40 &     0 &    50 &     5 &     0 \\ 
  6 &  $1/6$  &    40 &    40 &    50 &     5 &     0 \\ 
   \hline
\end{tabular}
\label{pars_table}
\end{table}


Let the parameters of $f$ be known. Figure~\ref{ex_mixture} shows the isodensity curves of the estimated \fmm for 
a random sample \textbf{X}. We want to cluster sample \textbf{X}.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.7\textwidth]{figures/partition-example-mixture.pdf} \\
 \end{tabular}
 \caption{Density of Gaussian mixture of 6 components. Each component's sample mean is represented by `+'.}\label{ex_mixture}
\end{center}
\end{figure}

The initial partition  $\mathcal{P}_6 = \{ \{1\},\{2\}, \{3\}, \{4\}, \{5\}, \{6\} \}$  by Equation~\ref{cluster_criteria} yields to six clusters where each component is associated to one cluster. In Figure~\ref{ex_one_one} we have separated the observations with respect to the cluster they were assigned to. The plot also includes the isodensity curves for the density modelling each cluster, in this case each cluster is modelled with a Gaussian distribution.

\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=\textwidth]{figures/partition-example-part6.pdf} \\
 \end{tabular}
 \caption{Sample separated into six clusters where each cluster is represented by a single component.}\label{ex_one_one}
\end{center}
\end{figure}

Using the posterior probabilities $\text{\textbf{T}}_{\mathcal{P}_s}$ and functions $\omega_{\text{cnst}}$ and $\lambda_{\Delta\text{Ent}}$ we obtained the sequential hierarchical partition given by 
\begin{equation}
\begin{array}{r c l}
\mathcal{P}_6 &=& \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\},\\
\mathcal{P}_5 &=& \{\{1\},\{2\}, \{3, 6\},\{4\},\{5\} \},\\
\mathcal{P}_4 &=& \{\{1, 4\},\{2\}, \{3, 6\}, \{5\} \}, \\
\mathcal{P}_3 &=& \{\{1, 2, 4\}, \{3, 6\}, \{5\} \}, \\
\mathcal{P}_2 &=& \{\{1, 2 , 4, 5\}, \{3, 6\} \},  \\
\mathcal{P}_1 &=& \{\{1, 2, 3, 4, 5, 6\}\}.
\end{array}
\label{hier_ex}
\end{equation}

In partition $\mathcal{P}_4 = \{\{1, 4\},\{2\}, \{3, 6\}, \{5\} \}$, the part $\{1, 4\}$ defines a single cluster, as does part \{3, 6\}. For this partition, using Equation~\ref{cluster_criteria} each observation $\m x_i$ is classified into one component. Figure~\ref{ex_two_one} shows the observations separated with respect to the cluster they were classified into, together with the isodensity curves defined by each component. In this case, clusters labelled $\{1,4\}$ and $\{3, 6\}$ are modelled by a mixture of two components. With partition $\mathcal{P}_4$ the clusters are modelled by $\fmm$s
\begin{itemize}
\item $f_{\{1,4\}} = \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_1, \m\Sigma_1) + \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_4, \m\Sigma_4)$, 
\item $f_{\{2\}} = \phi(\;\cdot\; ;  \m\mu_2, \m\Sigma_2)$, 
\item $f_{\{3,6\}} =  \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_3, \m\Sigma_3) + \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_6, \m\Sigma_6)$ and
\item $f_{\{5\}} = \phi(\;\cdot\; ;  \m\mu_5, \m\Sigma_5)$.
\end{itemize}

\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.65\textwidth]{figures/partition-example-part4.pdf} \\
 \end{tabular}
 \caption{Sample separated in four clusters where two clusters are represented by a mixture of two Gaussian components.}\label{ex_two_one}
\end{center}
\end{figure}

When $\omega_{prop}$ is combined with $\lambda_{\text{DEMP}_m}$, $\lambda_{\text{dist}}$, $\lambda_{\log}$ and $\lambda_{\text{prop}}$ we obtain the same hierarchical partition (Equation \ref{hier_ex}). Using $\omega_{prop}$ and $\lambda_{\text{DEMP}}$, the sequential hierarchical obtained only differs from the previous one  in partition $\mathcal{P}_5$, which is now $\mathcal{P}_5 = \{\{1, 4\},\{2\}, \{3\},\{5\},\{6\} \}$.

To decide the number of clusters we plot the $S$-values using different approaches (Figure~\ref{gaussian_Svalues} (top)). Using the Aitchison distance between posterior probabilities, Figure~\ref{gaussian_Svalues} (bottom) shows the Calinski-Harabasz (G1) and the Goodman and Kruskal (G2) indices \citep{milligan1985}, and the closeness to the vertex indices. We see that after merging components to model four clusters or less, the $S$-values are close to zero for all approaches. This indicates that the classifications defined with four, three, or two clusters are well separated. Both G1 and G2 have a local maximum in four clusters, indicating that with those methods to separate the data into four clusters is preferred. The closeness to the vertex criteria increases after merging into four clusters, indicating that the clusters are close to a vertex. Combining the information obtained with the $S$-values, the indices G1 and G2, and the closeness criteria, we then classify our data into four clusters using partition $\mathcal{P}_4$ as shown in Figure~\ref{ex_two_one}.

\begin{figure}[!t]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.65\textwidth]{figures/gaussian_Svalues.pdf} \\
   \includegraphics[width=0.65\textwidth]{figures/gaussian_statistics.pdf}
 \end{tabular}
\caption{Different criterias are shown to decide the number of clusters for the sample generated with a finite mixture of six Gaussian components. At the top of the figure the $S$-values of the different approaches are shown. In order by rows: the DEMP approach \citep{hennig2010methods} ($S_{\omega_{\text{prop}}, \lambda_{\text{DEMP}}}$) , the DEMP modified approach\citep{longford2014} ($S_{\omega_{\text{prop}}, \lambda_{\text{DEMP}_m}}$), the posterior probability approach ($S_{\omega_{\text{prop}}, \lambda_{\text{prop}}}$), the Aitchison distance with proportional weighing scaled ($\phi_{\text{dist}} \circ S_{\omega_\text{prop}, \lambda_{\text{dist}}}$), the difference of entropies approach scaled ($\phi_{\Delta\text{Ent}} \circ S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$) and the log-ratio approach with proportional weighing scaled ($\phi_{\log} \circ S_{\omega_{\text{prop}}, \lambda_{\log}}$). At the bottom part the distance based criteria for the posterior probability vectors are shown. In order: the Calinski-Harabasz and Goodman-Kruskal criteria, and the closeness to the vertex criteria, $\mathcal{V}$, given in Section~\ref{coda_clusters}.}\label{gaussian_Svalues}
\end{center}
\end{figure}

\subsection{Merging components in a mixture of multinomial distributions}\label{multinom_example}

Merging approaches presented in this article rely on the vector of posterior probabilities which can be calculated from any finite mixture model. Therefore, the merging generic approach introduced in Section~\ref{generic_merging} can be used for any family of \fmm, for example a finite mixture of multinomial distributions.

Pigs dataset can be obtained from the \emph{zCompositions}  \citep{palarea2015zcompositions} R package. The dataset contains count data of behavioural observations of a group of 29 sows. The sows were recorded over a five minute period in different moments and their activity was subsequently registered. Six locations were considered for each pig: straw bed (BED), half in the straw bed (HALF.BED), dunging passage (PASSAGE), half in the dunging passage (HALF.PASS), feeder (FEEDER) and half in the feeder (HALF.FEED).

We used \emph{mixtools} \citep{benaglia2009mixtools} to fit a multinomial mixture. Six components were identified as optimum according to the BIC criterion. Table~\ref{multinomial_pars} shows the parameters of each the components. Using these parameters we can compute the posterior probability matrix $\text{\textbf{T}}_{\mathcal{P}_6}$. Each observation is classified into one cluster following Equation~\ref{map_criteria} or Equation~\ref{cluster_criteria} with partition $\mathcal{P}_6 = \{\{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}\}$. In Figure~\ref{multinomial_mixture} we can see the bar plot for the observations classified into the same cluster.

\begin{table}[hp]
\caption{Parameters of a finite mixture of multinomial distributions adjusted to the Pigs data set. For component $j$, the mixing proportions are denoted by $\pi_j$ and the multinomial probabilities by $\left(\theta_{j1}, \dots, \theta_{j6}\right)$.}
\centering
\begin{tabular}{rrrrrrrr}
  \hline
 Comp.& $\pi_j$ & $\theta_{j1}$ & $\theta_{j2}$ & $\theta_{j3}$ & $\theta_{j4}$ & $\theta_{j5}$ & $\theta_{j6}$ \\ 
  \hline
  1 & 0.0695 & 0.0103 & 0.0000 & 0.2874 & 0.0103 & 0.6867 & 0.0052 \\ 
  2 & 0.1710 & 0.0144 & 0.0000 & 0.0717 & 0.0020 & 0.9057 & 0.0062 \\ 
  3 & 0.0699 & 0.0817 & 0.0102 & 0.1390 & 0.0000 & 0.7538 & 0.0154 \\ 
  4 & 0.1724 & 0.1567 & 0.0082 & 0.7835 & 0.0021 & 0.0454 & 0.0041 \\ 
  5 & 0.0345 & 0.9485 & 0.0000 & 0.0309 & 0.0000 & 0.0206 & 0.0000 \\ 
  6 & 0.4828 & 0.7408 & 0.0147 & 0.1694 & 0.0074 & 0.0626 & 0.0052 \\  
   \hline
\end{tabular}
\label{multinomial_pars}
\end{table}

\begin{figure}[hpb]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.95\textwidth]{figures/multinomial_mixt_all.pdf} \\
 \end{tabular}
 \caption{Components after adjusting a six mixture of multinomial distributions. For each cluster, the relative amount of time seen in each location is shown.}\label{multinomial_mixture}
\end{center}
\end{figure}

Using $\omega_{\text{dich}}$ or  $\omega_{\text{prop}}$ with $\lambda_{\text{dist}}$ or $\lambda_{\log}$, we obtained the hierarchical structure partition given by
\begin{equation}
\begin{array}{r c l}
 \mathcal{P}_6&=& \;\; \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}, \\
 \mathcal{P}_5&=& \;\; \{\{1\},\{2\},\{3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_4&=& \;\; \{\{1,2\},\{3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_3&=& \;\; \{\{1,2,3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_2&=& \;\; \{\{1,2,3\},\{4,5,6\}\}, \\ 
 \mathcal{P}_1&=& \{\{1,2,3,4,5,6\}\}.
\end{array}
\label{hier_ex_multinomial}
\end{equation}

Other criteria such as $S_{\omega_{\text{csnt}},\lambda_{\Delta\text{Ent}}}$,  $S_{\omega_{\text{prop}},\lambda_{\text{DEMP}}}$,  and $S_{\omega_{\text{prop}},\lambda_{\text{prop}}}$ differed only in partitions $\mathcal{P}_5$ and $\mathcal{P}_4$.  That is, these methods preferred to first merge the part $\{1,2,3\}$ obtaining partitions $\mathcal{P}_5 = \{\{1\},\{2, 3\},\{4\},\{5\} ,\{6\}\}$ and $\mathcal{P}_4 = \{\{1,2,3\},\{4\},\{5\},\{6\}\}$.

\begin{figure}[thpb]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.65\textwidth]{figures/multinomial_Svalues_all.pdf} \\
  \includegraphics[width=0.65\textwidth]{figures/multinomial_statistics.pdf} 
 \end{tabular}
\caption{Different criteria are shown to decide the number of clusters for the pigs sample. At the top of the figure the $S$-values of different approaches are shown. In order by rows: the DEMP approach \citep{hennig2010methods} ($S_{\omega_{\text{prop}}, \lambda_{\text{DEMP}}}$) , the DEMP modified approach \citep{longford2014} ($S_{\omega_{\text{prop}}, \lambda_{\text{DEMP}_m}}$), the posterior probability approach ($S_{\omega_{\text{prop}}, \lambda_{\text{prop}}}$), the Aitchison distance with proportional weighing scaled ($\phi_{\text{dist}} \circ S_{\omega_\text{prop}, \lambda_{\text{dist}}}$), the difference of entropies approach scaled ($\phi_{\Delta\text{Ent}} \circ S_{\omega_{\text{cnst}}, \lambda_{\Delta\text{Ent}}}$) and the log-ratio approach with proportional weighing scaled ($\phi_{\log} \circ S_{\omega_{\text{prop}}, \lambda_{\log}}$). At the bottom part the distance based criteria for the posterior probability vectors are shown. In order: the Calinski-Harabasz and Goodman-Kruskal criteria, and the closeness to the vertex criteria, $\mathcal{V}$, given in Section~\ref{coda_clusters}.}\label{multinomial_statistics}
\end{center}
\end{figure}

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.95\textwidth]{figures/multinomial_clust3_all.pdf} \\
 \end{tabular}
 \caption{Pig dataset components after clustering the six mixture components in a 3-\fmm. For each cluster, the relative amount of time seen in each location is shown.}\label{multinomial_clust3}
\end{center}
\end{figure}

To decide the number of clusters we plot the $S$-values (Figure~\ref{multinomial_statistics} (top)). The distance based statistics are ploted in Figure~\ref{multinomial_statistics} (bottom). In this case it is difficult to decide the final number of clusters. The $S$-values of DEMP, DEMP modified and proportion approaches suggest separating the sample into four clusters. In contrast, the Aitchison distance approach and the log-ratio approach suggest five clusters, and with the difference of entropies is difficult to decide. Using the distance based criterias we see that the Calinski-Harabasz and Goodman-Kruskal criteria suggest separating the sample into either three or four clusters. Finally, the closeness to the vertex criteria suggests that the sample is close to the vertex once the sample has been merged into three clusters. With all this information at hand, it seems reasonable to chose either three or four clusters.  Figure~\ref{multinomial_clust3}  shows the sample after separating into three clusters. The first cluster contains the mixture components one, two and four; all of them represented by sows with a high amount of feeding time. The second cluster contains the mixture components three and five; this cluster is characterized by sows with high amounts of bed time. Finally, the third cluster is only formed by the component six, which equates to a higher amount of passage time. The reader interested in how to compare groups in compositional data sets can consult \cite{MDM15manova}.


\section{Final remarks}\label{remarks}

 When \fmm is used in clustering, the question \textit{``is a cluster determined by a unique component?''} emerges. Different authors have proposed scenarios where it seems reasonable to argue that a cluster can be better modelled by more than one single component, or equivalently, modelled by an \fmm itself. In these same scenarios, the approaches proposed in this article may be of interest.

In this article we propose a generic approach to build a hierarchy of mixture components, which relies only on the posterior probability vectors, and therefore, it is independent from the family of probability distributions. This generic approach allows us to both integrate some criterias that had appeared earlier in the literature and to propose some new techniques. As the posterior probability vectors belong to the simplex sampe space, we use the log-ratio methodology developed for this type of vector to develope these new techniques. All the methods described in this article can be applied to any \fmm. We have illustrated the approach with both a numerical and categorical sample. 

To decide final number of clusters we have proposed combining the information given by the scaled $S$-values and we have also developed some criterias based on the location of the posterior probabilities. For the latter we propose the Calinski-Harabasz and the Goodman and Kruskal incides using Aitchsiosn distances between posterior probabilities vectors and the closeness to the vertex criteria. To the best of our knowledge it is the first time that these two options have been proposed to study cluster structure. This new approach allows the structure of numerical and categorical data sets based on the considered model to be studied.

Finally, the log-ratio approaches introduced in this article (Section~\ref{logratio_section}) and the indices defined on the posterior probabilities (Section~\ref{coda_clusters}) use the geometric structure of the Simplex space. Working with the posterior probabilities as an element of the Simplex space allows  the results obtained here to be extended to other methods. For example, in fuzzy-clustering, it would be interesting to analyse if  the role played by the weights is equivalent to the role played by the posterior probabilities. 




%%% BIBLIOGRAPHY

%\bibliographystyle{apalike}
\begin{thebibliography}{}


\bibitem[Aitchison, 1986]{aitchison1986statistical}
Aitchison J (1986).
\newblock {\em {The Statistical Analysis of Compositional Data}}.
\newblock Monographs on Statistics and Applied Probability. Chapman \& Hall Ltd., London (UK). Reprinted (2003) with additional material by The Blackburn Press, Caldwell, NJ

\bibitem[Aitchison, 2002]{aitchison2002simplicial}
Aitchison J (2002).
\newblock {\em {Simplicial inference}}.
\newblock {\em Algebraic Methods in Statistics and Probability}, \textbf{287}, 1--22.

\bibitem[Baudry et~al., 2010]{baudry2010combining}
Baudry JP, Raftery AE, Celeux G, Lo K and Gottardo R (2010).
\newblock {Combining Mixture Components for Clustering}.
\newblock {\em Journal of Computational and Graphical Statistics}, \textbf{19}(2), 332--53.

\bibitem[Benaglia et~al., 2009]{benaglia2009mixtools}
Benaglia T, Chauveau D, Hunter DR and Youn R (2009).
\newblock {mixtools: An R Package for Analyzing Finite Mixture Models}.
\newblock {\em Journal of Statistical Software}, \textbf{32}(6), 1--29.

\bibitem[Fraley and Raftery, 1998]{fraley1998how}
Fraley C and Raftery AE (1998).
\newblock {How many clusters? Answers via model-based cluster analysis}.
\newblock {\em The Computer Journal}, \textbf{41}, 578--88.

\bibitem[Fraley and Raftery, 2002]{fraley2002model}
Fraley C and Raftery AE (2002).
\newblock {Model-Based Clustering, Discriminant Analysis, and Density Estimation}.
\newblock {\em Journal of the American Statistical Association}, \textbf{97}(458), 611--31.

\bibitem[Frey and Dueck, 2007]{frey2007}
Frey BJ and Dueck D (2007).
\newblock {Clustering by passing messages between data points}.
\newblock {\em Science}, \textbf{315}, 972--76.

\bibitem[Hennig, 2010]{hennig2010methods}
Hennig C (2010).
\newblock {Methods for merging Gaussian mixture components}.
\newblock {\em Advances in Data Analysis and Classification}, \textbf{4}(1), 3--34.

\bibitem[Keribin, 1998]{keribin1998consistent}
Keribin C (1998).
\newblock {Consistent estimate of the order of mixture models}.
\newblock {\em Comptes Rendues de l’Academie des Sciences, Série I-Mathématiques}, \textbf{326}, 243--48.


\bibitem[Keribin, 2000]{keribin2000consistent}
Keribin C (2000).
\newblock {Consistent Estimation of the Order of Mixture Models}.
\newblock {\em Sankhy\={a}: The Indian Journal of Statistics, Series A}, \textbf{62}(1), 49--66.

\bibitem[Lee and Cho, 2004]{lee2004combining}
Lee HJ and Cho S (2004).
\newblock {Combining Gaussian Mixture Models}.
\newblock In Yang, Z, Yin, H, and Everson, R, editors, {\em Intelligent Data
  Engineering and Automated Learning – IDEAL 2004 SE - 98}, volume 3177 of
  {\em Lecture Notes in Computer Science}, pages 666--71. Springer Berlin
  Heidelberg.

\bibitem[Longford and Bartosova, 2014]{longford2014}
Longford NT and Bartosova J (2014).
\newblock {A confusion index for measuring separation and clustering}.
\newblock {\em Statistical Modelling}, \textbf{14}(3), 229--55.


\bibitem[Martín-Fernández et~al., 2015]{MDM15manova}
Martín-Fernández JA, Daunis-i-Estadella J and  Mateu-Figueras G (2015).
\newblock {On the interpretation of differences between groups for compositional data}.
\newblock {\em SORT}, \textbf{39}(2), 231--52.

\bibitem[McLachlan and Rathnayake, 2014]{mclachlan2014components}
McLachlan GJ and Rathnayake S (2014).
\newblock {On the number of components in a Gaussian mixture model}.
\newblock {\em Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},  \textbf{4}, 341--55.

\bibitem[Melnykov, 2013]{melnykov2013distribution}
Melnykov V (2013).
\newblock {On the Distribution of Posterior Probabilities in Finite Mixture Models with Application in Clustering}.
\newblock {\em Journal of Multivariate Analysis}, \textbf{122}, 175--89.

\bibitem[Milligan, 1985]{milligan1985}
Milligan GW and Cooper MC (1985) 
\newblock {An examination of procedures for determining the number of clusters}. 
\newblock {\em Psychometrika}, \textbf{50}, 159-79.

\bibitem[Palarea-Albaladejo et~al., 2012]{palarea2012dealing}
Palarea-Albaladejo J, Martín-Fernández JA and Soto JA (2012).
\newblock {Dealing with Distances and Transformations for Fuzzy C-Means Clustering of Compositional Data}.
\newblock {\em Journal of Classification}, \textbf{29}(2), 144--69.


\bibitem[Palarea-Albaladejo and Martín-Fernández, 2015]{palarea2015zcompositions}
Palarea-Albaladejo J and Martín-Fernández JA (2015).
\newblock {zCompositions - R packages for multivariate imputation of nondetecteds and zeros in compositional data sets}.
\newblock{\em Chemometrics and Intelligent Laboratory Systems}, \textbf{143}, 85--96.

\bibitem[Pastore and Tonellato, 2013]{pastore2013merging}
Pastore A and Tonellato SF (2013).
\newblock {A Merging Algorithm for Gaussian Mixture Components}.
\newblock {\em SSRN Electronic Journal, University Ca' Foscari of Venice, Dept. of Economics Research Paper Series No. 04/WP/2013: http://dx.doi.org/10.2139/ssrn.2233307}.

\bibitem[Pawlowsky-Glahn and Egozcue, 2001]{PE2001geo}
Pawlowsky-Glahn V and Egozcue JJ (2001).
\newblock {Geometric approach to statistical analysis on the simplex}.
\newblock {\em SERRA}, \textbf{15}, 384-98.

\bibitem[Punzo, 2014]{punzo2014flexible}
Punzo A (2014).
\newblock {Flexible mixture modelling with the polynomial Gaussian cluster-weighted model}.
\newblock {\em Statistical Modelling}, \textbf{14}(3), 257--91.

\bibitem[{R development core team(2015)}]{R}
R development core team (2015) 
\newblock {R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna,
  Austria}. 
 \newblock {URL \em{http://www.r-project.org}}.



\end{thebibliography}

%%%%%%%%%%% END SPACING
%\end{spacing}

\end{document}
