%\documentclass[10pt, a4paper]{article}
\documentclass[submit]{smj}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, bbm}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage{apalike}
\usepackage{relsize}
\usepackage{array}
\usepackage{multirow}
%\usepackage{showlabels}
\usepackage{setspace}
\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{soul}
%Add line numbering
%Line numbering can be incorporated by using the lineno package. Add these statements in the preamble:
%\usepackage{lineno}
%\linenumbers

%\usepackage[nomarkers,figuresonly]{endfloat}

\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{prob}{Problem}
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
%%%%% bold symbol in math enviornment
\newcommand{\m}[1]{\boldsymbol{#1}}

\newcommand{\fmm}{\textsc{fmm}\xspace}
\newcommand{\X}{\text{\textbf{X}}}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} 

\Title{Merging the components of a finite mixture using  posterior probabilities}
\TitleRunning{Merging the components of \fmm}
\Author{Marc Comas-Cufí\Affil{1}, Josep A. Martín-Fernández\Affil{1}, Glòria Mateu-Figueras\Affil{1}}
\AuthorRunning{Marc Comas-Cufí \textrm{et al.}}

\Affiliations{
\item Department of Computer Sciences, Applied Mathematics and Statistics, 
      Polytechnic School, 
      University of Girona,
      Spain
}

\CorrAddress{Josep A. Martín-Fernández, Department of Computer Science, Applied Mathematics and Statistics,
      University of Girona, P-IV, Campus Montilivi, E-17071 Girona, Spain}
\CorrEmail{josepantoni.martin@udg.edu}
\CorrPhone{(+34)\;972\; 418\;426}
\CorrFax{(+34)\;972\;418\;792}

\Abstract{
Methods in parametric cluster analysis commonly assume data can be modelled by means of a finite mixture of distributions. Different authors show that associating each mixture component to one cluster is frequently misleading because different mixture can components overlap forming a unique cluster. In the literature, we can find different proposals to construct the clusters by merging components using the posterior probabilities. In this paper a generic approach to build a hierarchy of mixture components that generalises some techniques earlier proposed in the literature is presented.  Using this proposal a new technique based in the log-ratio of posterior probabilities is introduced. Moreover, to decide the final number of clusters two new methods are introduced. Simulated and real datasets are used to illustrate this methodology.
}

\Keywords{
Model-based clustering; Mixture model; Hierarchical clustering; Merging components; Log-ratio; Simplex
}
\begin{document}

%\begin{spacing}{1.9}
%%%%%%% BEGIN SPACING


\pagenumbering{arabic}

\maketitle

\section{Introduction}


% Mixture models
A common approach in parametric cluster analysis assumes data can be modelled by means of a \emph{finite mixture of distributions} \citep{fraley2002model, punzo2014flexible}, also called \emph{finite mixture model} (\fmm). A \fmm is a probability distribution whose probability density function (pdf) is a linear combination of different distributions with same domain $\mathbb{X}$. More precisely, the pdf $f$ of a \fmm is
\begin{equation}\label{mixt}
f(\;\cdot\; ; \pi_1, \dots, \pi_k, \m\Theta) = \pi_1 f_1(\;\cdot\; ; \m\theta_1) + \dots + \pi_k f_k(\;\cdot\; ; \m\theta_k),
\end{equation}
where $\m\Theta =\{ \m\theta_1, \dots, \m\theta_k\}$, $\m\theta_j$ are the parameters of pdf $f_j$, $1\leq j \leq k$, and $\pi_j$ is the ``weight'' of component $f_j$. Restriction $\sum_{\ell = 1}^k \pi_\ell = 1$ guarantees that  $\int_{\mathbb{X}}f = 1$. Originally, the clustering algorithm based on a \fmm follows two steps:
\begin{enumerate}
\item to calculate estimates $\hat{\pi}_1, \dots, \hat{\pi}_k$ and $\hat{\m\Theta}$ of parameters $\pi_1, \dots, \pi_k$ and $\m\Theta$ using a sample $\X=\{\m x_1, \dots, \m x_n\}$ and
\item to classify each observation $\m x_i \in \X$ to a cluster c, $1\leq c \leq k$, according to the criterium of maximising the posterior probability
\[
\tau_{ij}= \frac{ \hat{\pi}_j f_j(\m x_i ; \hat{\m\theta}_j) }{\sum_{\ell=1}^k \hat{\pi}_\ell f_\ell(\m x_i ; \hat{\m\theta}_\ell) },
\]
that is, one observation $\m x_i$ is classified to cluster $c$ if
\begin{equation}\label{map_criteria}
c = \argmax_{j=1}^k \tau_{ij}.
\end{equation}
\end{enumerate}
Note that in this process, the number of clusters is equal to the number of mixture components, i.e.  $k$. \cite{mclachlan2014components} presents a recent review of different approaches used to decide the value of $k$. From the reviewed approaches, we use the Bayesian Information Criterion (BIC) to fit the number of mixture components. The use of BIC to decide the number of components is justified by \cite{keribin1998consistent, keribin2000consistent}, who shows that under certain regularity conditions, it estimates the number of mixture components consistently. \cite{fraley1998how} show the BIC is effective as a model selection criteria  at practical level.

On the other hand, \cite{lee2004combining}, \cite{hennig2010methods}, \cite{baudry2010combining}, \cite{melnykov2013distribution} and \cite{pastore2013merging} propose to separate the concepts of cluster and mixture component. The authors show that associating each mixture component to one cluster can be misleading because frequently different mixture components are so overlapped that they can be considered as a unique cluster. In other words, one cluster could be formed by the result of merging two or more different mixture components. According to this approach, the \fmm clustering algorithm is completed with a third step:

\begin{itemize}
\item[3.] to analyse which of the $k$ mixture components should be merged to form $k'$ clusters, $k' \leq k$.
\end{itemize}

The crucial point of this new step is how to decide which components have to be merged. Importantly, the underlying finite mixture do not change after merging two components. Therefore, for a given sample the likelihood function remains invariant. This fact makes impossible to decide which components form a single clusters in terms of the likelihood or through the BIC criteria \citep{hennig2010methods}. The approaches presented in this paper are based on the posterior probabilities which are obtained after adjusting a \fmm. In this article we introduce a generic approach that generalises methods given by \cite{baudry2010combining}, \cite{hennig2010methods} and \cite{longford2014}. In addition, with this new approach, the analyst can define its own criteria, for example, we introduce a criteria based on log-ratio transformations of posterior probabilities \citep{aitchison1986statistical}. 
Using this generic approach for merging components one can also build a hierarchy over the set of mixture components. At the first level of this hierarchy we are modelling $k$ clusters where each cluster is modelled by one component, in the second level we have $k-1$ clusters where one cluster is modelled by two components. At the following levels subsequent clusters are modelled by the merging of different components. The final level contains one cluster (the original sample) modelled by a single component (the original mixture).

The paper is organised as follows: in Section~\ref{definitions} some definitions and notations used throughout this paper are introduced. In Section~\ref{generic_merging} the general merging criteria is presented. It is shown that the most important techniques from literature reduce to this new approach. Using this proposal, Section~\ref{logratio_section} presents a new family of techniques based on the log-ratio methodology. In Section~\ref{number_clusters}, two heuristic methods to decide the final number of clusters are proposed. In Section~\ref{merging_examples_dist}, we present two examples to illustrate the algorithm with different types of mixture distributions. To conclude, final remarks are given in Section~\ref{remarks}.

\section{Definitions and notation}\label{definitions}

%
Let $\mathcal{I}^k = \{1, \dots, k\}$ be a set of natural numbers to indicate the components of a \fmm. A \emph{partition} of $\mathcal{I}^k$ of size $s$, $\mathcal{P}_s$, is the disjoint union of subsets $I_p$, called \emph{parts}. More formally, $\mathcal{P}_s$ is a set with $s$ subsets $I_p$  that $\bigcup_{I_p \in \mathcal{P}_s} I_p = \mathcal{I}^k$ and for any two parts $I_a, I_b \in \mathcal{P}_s$ with $a \neq b$, $I_a \cap I_b = \emptyset$ holds. Given a partition  $\mathcal{P}_s$, the pdf $f$ of a \fmm (Equation~\ref{mixt}) can be written as
\begin{equation}
f = \pi_{I_1} f_{I_1}(\;\cdot\;; \m\Theta) + \dots + \pi_{I_s} f_{I_s}(\;\cdot\;; \m\Theta),
\label{mixt_part}
\end{equation}
where $f_{I_p}(\;\cdot\;;  \m\Theta) = \sum_{j \in I_p} \frac{\pi_j}{\pi_{I_p}} f_j(\;\cdot\; ; \m\theta_j)$ and $\pi_{I_p} = \sum_{\ell \in I_p} \pi_\ell$. Note that using this notation each $f_{I_p}(\;\cdot\;;  \m\Theta)$ is also a \fmm. Because each part $I_p$ defines a single component $f_{I_p}$, when there is no confusion, we use $I_p$ referring either to the part $I_p$ or to the component $f_{I_p}$. Note that, given $k$ components of a \fmm $f$, there is $B_k$ different ways to express the mixture  $f$ in terms of a partition.\footnote{$B_k$ is the $k$-th Bell number defined recursively as $B_0 = 1$ and $B_k = \sum_{i=0}^{k-1} \binom ki B_{i-1}$.}



A \emph{hierarchical sequence of partitions of $\mathcal{I}^k$} is a sequence $\mathcal{P}_1, \dots, \mathcal{P}_k$ verifying that
\begin{itemize}
\item $\mathcal{P}_1$ is the one-part partition $\mathcal{P}_1 = \{ \mathcal{I}^k \}$,
\item if a part $I_p \in \mathcal{P}_{s-1}$ then either there is a part $I_a \in \mathcal{P}_{s}$ with $I_p = I_a$ or there are two parts $I_a, I_b \in \mathcal{P}_s$ with $I_p = I_a \cup I_b$, and
\item $\mathcal{P}_k= \{ \{1\},\{2\}, \dots, \{k\} \}$.
\end{itemize}



One can extend Equation~\ref{map_criteria} in terms of partitions. Indeed, let $\X = \{\m x_1,\dots, \m x_n\}$ be a sample formed by observations of $\mathbb{X}$. Given a partition $\mathcal{P}_s = \{ I_1, \dots, I_s \}$, we define the posterior probability of $\m x_i$ being classified to part $I_p\in \mathcal{P}_{s}$ as
\[
\tau_{i I_p} =  \frac{ \hat{\pi}_{I_p} f_{I_p}(\m x_i; \hat{\m\Theta}) }{\sum_{\ell=1}^s \hat{\pi}_{I_\ell} f_{I_\ell}(\m x_i; \hat{\m\Theta})}
\]
where $\hat{\pi}_{I_p} = \sum_{\ell \in I_p} \hat{\pi}_\ell$.

Then, we define the posterior probability vector associated to observation $\m x_i$ as
\begin{equation}\label{ppv}
\m\tau_{i \mathcal{P}_s} = \left(\tau_{i I_1} , \dots, \tau_{i I_s}  \right).
\end{equation}
The posterior probability vector $\m \tau_{i \mathcal{P}_s}$ denotes the conditional probability that $\m x_i$ arises from mixture components $f_{I_1}, \dots, f_{I_s}$. Since $\mathcal{P}_s$ is a partition $\sum_{p=1}^s \tau_{i I_p} = 1$ holds  for $1 \leq i \leq n$. Similarly to Equation~\ref{map_criteria}, the posterior probability vectors $\m\tau_{i \mathcal{P}_s}$ can be used to classify $\m x_i \in \X$ to the cluster $c$, $1\leq c\leq s$, if
\begin{equation}\label{cluster_criteria}
c= \argmax_{p=1}^s \{ \tau_{i I_p} \}.
\end{equation}

Let $\text{\textbf{T}}_{\mathcal{P}_s}$ be the matrix with $n$ rows and $s$ columns formed by the $n$ vectors of posterior probabilities $\m \tau_{i\mathcal{P}_s}$ associated to partition $\mathcal{P}_s$. For example, $\text{\textbf{T}}_{\mathcal{P}_k}$ is the initial matrix, when the number of components and clusters are equal. Importantly, by Equation \ref{ppv}, any matrix $\text{\textbf{T}}_{\mathcal{P}_s}$ can be obtained from matrix $\text{\textbf{T}}_{\mathcal{P}_k}$ respectively aggregating the corresponding columns of parts $I_1, \dots, I_s$.


\section{Generic approach}\label{generic_merging}

\subsection{General merging criteria}\label{merging_criteria}

Let $\text{\textbf{X}} = \{\m x_1,\dots,\m x_n \}$ be a sample defined in  a domain $\mathbb{X}$ and let $f$ be a \fmm with $k$ components defined on $\mathbb{X}$ (Equation~\ref{mixt}). Given a partition $\mathcal{P}_s = \{I_1, \dots, I_s\}$, let $\m\tau_{i \mathcal{P}_s}= \left( \tau_{i I_1} , \dots, \tau_{i I_s}  \right)$ be the posterior probability vector associated to observation $\m x_i$, $1\leq i \leq n $.

For a partition $\mathcal{P}_s$  and matrix of posterior probabilities $\text{\textbf{T}}_{\mathcal{P}_s}$ we propose to merge the parts $I_a, I_b \in \mathcal{P}_s$ maximising the weighted mean
\begin{equation}\label{unifying_equation}
S_{\omega, \lambda}( \text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) = \frac{\sum_{i=1}^n \omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b) \; \lambda(\m\tau_{i \mathcal{P}_s}, I_a, I_b)}{\sum_{i=1}^n \omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b) },
\end{equation}
where $\lambda(\m\tau_{i \mathcal{P}_s}, I_a, I_b)$ is a real valued function and $\omega(\m\tau_{i \mathcal{P}_s}, I_a, I_b)$ is a non-negative function.  Function $\lambda(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$ have the role of an utility function. It measures our preferences to consider components $f_{I_a}$ and $f_{I_b}$ as a single component, and therefore, to model a single cluster by means of the merged component $f_{I_a \cup I_b}$. Function $\omega(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$ is a weight function. It permits to modify the influence that each posterior probability vector $\m\tau_{i \mathcal{P}_s}$ has in Equation~\ref{unifying_equation} for parts $I_a$ and $I_b$. The behaviour of function $S_{\omega, \lambda}$ is completely determined by the choice of function $\omega$ and $\lambda$. Importantly, function $S_{\omega, \lambda}$ has the same image than function $\lambda$.


Starting from partition $\mathcal{P}_k = \{ \{1\}, \dots, \{k\} \}$, where the number of clusters is equal to the number of components, two parts are merged according to the maximum of Equation~\ref{unifying_equation}. We refer to this maximum as \emph{the $S$-value}. Iteratively repeating this process, the algorithm builds an agglomerative hierarchical sequence of partitions until partition $\mathcal{P}_1$ is obtained. Remarkably, by the definition of functions $\omega$ and $\lambda$, the process only depends on the posterior probability vectors $\text{\textbf{T}}_{\mathcal{P}_s}$.

For the initial partition $\mathcal{P}_k$, we need to evaluate $k^2-k$ times function $S_{\omega, \lambda}$ to obtain  the corresponding $S$-value. The process is repeated from partition $\mathcal{P}_{k-1}$ to $\mathcal{P}_1$, evaluating a maximum of $\frac{k^3-k}{3}$ times the function $S_{\omega, \lambda}$. These quantities are reduced by half when function $S_{\omega, \lambda}$ is symmetric as regards to $I_a$ and $I_b$. 

\subsection{Minimising the final entropy}
\label{entropy_section}

\cite{baudry2010combining} propose an algorithm to build a hierarchical sequence of partitions based on the concept of entropy. The Shannon entropy of a posterior probability vector $\m\tau_{i \mathcal{P}_s} = \left( \tau_{i I_1} , \dots, \tau_{i I_s}  \right)$ is
\[
\text{Ent}( \m\tau_{i \mathcal{P}_s} ) = -\sum_{j=1}^s \tau_{i I_j}  \log(\tau_{i I_j} ).
\]

The entropy can be interpreted as a measure of similarity between a probability vector $\m\tau_{i \mathcal{P}_s}$ and the probability vector $\m\tau^0_{s}=\left(\frac{1}{s}, \dots, \frac{1}{s}\right)$, taking the maximum value $-\log(\frac{1}{s})$ when $\m\tau_{i \mathcal{P}_s}=\m\tau^0_{s}$. Given a partition $\mathcal{P}_s = \{ I_1, \dots, I_s\}$ the algorithm iteratively merges  the two mixture components optimising the overall entropy. Let $\mathcal{P}_{s-1}^{I_a\cup I_b}$ be the partition obtained after merging components $I_a$ and $I_b$ from $\mathcal{P}_s$. The parts $I_a$ and $I_b$ merged minimise expression
\[
\sum_{i=1}^n \text{Ent}( \m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}} ).
\]

According to \cite{baudry2010combining}, minimising previous expression is equivalent to maximise the loss of entropy, that is, to maximise the sum of differences
\[
\sum_{i=1}^n  \left\{ \text{Ent}( \m\tau_{i \mathcal{P}_s} ) - \text{Ent}( \m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}} ) \right\}.
\]
which can be written only in terms of $\tau_{i I_a}$  and $\tau_{i I_b}$ as
\begin{equation}\label{entropy}
\sum_{i=1}^n   \left\{(\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \left[\tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b})\right] \right\}.
\end{equation}
Note that using our general merging criteria we can define function $\lambda_{\,\Delta\text{Ent}}$ as
\[
\lambda_{\,\Delta\text{Ent}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) :=  (\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \left[ \tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b}) \right],
\]
and function $\omega_{\text{cnst}}$ as a constant, for example 
\[
\omega_{\text{cnst}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := 1.
\]

When assuming that $\omega_{\text{cnst}}$ is constant, we are considering each observation equally important to compute the $S$-value. That is, the weighting is the same regardless parts $I_a$ and $I_b$ we are willing to merge. In this case, the $S$-values (Equation~\ref{unifying_equation}) take the form
\[
\begin{split}
S_{\text{cnst}, \Delta\text{Ent}}( \text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) = \frac{1}{n} \sum_{i=1}^n & \left\{(\tau_{iI_a}+\tau_{iI_b}) \log(\tau_{iI_a} + \tau_{iI_b}) - \right.\\ 
&\quad \left.\left[ \tau_{iI_a} \log(\tau_{iI_a}) + \tau_{iI_b} \log(\tau_{iI_b}) \right]\right\},
\end{split}
\]
i.e. the average of loss of entropy.

Note that in this approach function $S_{\text{cnst}, \Delta\text{Ent}}$ is symmetric with respect to $I_a$ and $I_b$. Therefore, for partition $\mathcal{P}_k$ we only need to evaluate $S_{\text{cnst}, \Delta\text{Ent}}$ $\frac{k^2-k}{2}$ times. Note also that for partition $\mathcal{P}_{k-1}$ we only need to update the value of $S_{\text{cnst}, \Delta\text{Ent}}$ for $k-2$ different values. To calculate all the $S$-values we only need to evaluate $(k-1)^2$ times function $S_{\text{cnst}, \Delta\text{Ent}}$. 

\subsection{Maximising the misclassification probability}
\label{missclassification_section}

\cite{hennig2010methods} proposes to merge the components $I_a$ and $I_b$ from $ \mathcal{P}_s$ that maximise \emph{the probability of classifying to component $I_b$ an observation generated from component $f_{I_a}$}. To estimate this probability,  \cite{hennig2010methods} introduces a consistent estimator, the Directed Estimated Misclassification Probabilities (DEMP) defined as
\[
\frac{ \frac{1}{n} \sum_{i=1}^n {\tau_{iI_a} \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)}}{ \hat{\pi}_{I_a}},
\]
where $\mathbbm{1}\left( \cdot \right)$ is the indicator function. Because $ \hat{\pi}_{I_a} = \frac{1}{n} \sum_{i=1}^n \tau_{iI_a}$, the estimator DEMP can be written in terms of posterior probability as
\begin{equation}\label{demp_criteria}
\frac{ \sum_{i=1}^n {\tau_{iI_a} \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)}}{\sum_{i=1}^n \tau_{iI_a} }.
\end{equation}

Note that when parts $I_a$ and $I_b$ overlap, Equation~\ref{demp_criteria} takes higher values because many observations generated by $I_a$ takes a largest posterior probability $\tau_b$.

Using our general merging criteria, the estimator DEMP (Equation~\ref{demp_criteria}) is equivalent to set functions $\omega$ and $\lambda$ as 
\begin{equation}\label{lambda_demp}
\lambda_{\text{DEMP}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \mathbbm{1}\left( \forall j\; \tau_{i I_{b}} \geq \tau_{iI_j} \right)
\end{equation}
and
\[
\omega_{\text{prop}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) :=  \tau_{iI_a}.
\]

In this approach function $\lambda_{\text{DEMP}}$ is giving preference to observations $\m x_i$ classified to part $I_b$. Moreover, $\omega_{\text{prop}}$ function is weighing higher those observations with high posterior probability $\tau_{iI_a}$. This approach permits to calculate the $S$-value from $I_a$ to $I_b$ measuring our preference to merge $I_a$ into $I_b$. Note that in this case function  $S_{\text{prop}, \text{DEMP}}$ is not symmetric, i.e. $S_{\text{prop}, \text{DEMP}}(\text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b) \neq S_{\text{prop}, \text{DEMP}}(\text{\textbf{T}}_{\mathcal{P}_s},  I_b,  I_a)$. Therefore, to calculate the $S$-values we need to evaluate $2 (k-1)^2$ times function $S_{\text{prop}, \text{DEMP}}$. In addition, this lack of symmetry indicates that the merging process is extended to a type of absorbation process of one component by the other.

A variation of \cite{hennig2010methods} approach is proposed by \cite{longford2014}. Instead of considering function $\lambda_{\text{DEMP}}$ given by Equation~\ref{lambda_demp}, in this case the preference for merging $I_a$ into $I_b$ is given by
\begin{equation}\label{lambda_dempM}
\lambda_{\text{DEMP}_m}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \frac{\tau_{iI_b}}{\tau_{iI_a} + \tau_{iI_b}}.
\end{equation}

Similarly to function $\lambda_{\text{DEMP}}$, function $\lambda_{\text{DEMP}_m}$ gives high score to observations with high posterior probability $\tau_{iI_b}$. In this case, $\lambda_{\text{DEMP}_m}$ is the probability of being generated by component $f_{I_b}$ conditioned being generated by either $f_{I_a}$ or $f_{I_b}$. Again, function $\lambda_{\text{DEMP}_m}$ makes function $S_{\omega, \lambda_{\text{DEMP}_m}}$ not symmetric. \textcolor{red}{The function $\lambda_{\text{DEMP}_m}$, having the codomain $[0,1]$, increases when the probability $\tau_{iI_b}$ increases, and decreases when the probability $\tau_{iI_a}$ is increased.}


\section{Other criteria: the logratio approach}\label{logratio_section}


The generic expression of $S_{\omega, \lambda}(\text{\textbf{T}}_{\mathcal{P}_s},  I_a,  I_b)$ (Equation~\ref{unifying_equation}) permits to extend the criteria for merging components to expressions defined by the analyst. For example, in \cite{hennig2010methods} and \cite{longford2014} we prefer observations with high $\tau_{I_b}$. In this context, it may be reasonable to consider
\[
\lambda_{\text{prop}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \tau_{iI_b}.
\]
In this case, function $\lambda_{\text{prop}}$ combined with $\omega_{\text{prop}}$ results in an algorithm with an easily computable function $S_{\text{prop}, \text{prop}}$. As an alternative to function $\omega_{\text{prop}}$ we can define 
\[
\omega_{\text{dich}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := \mathbbm{1}\left( \forall j\; \tau_{i I_{a}} \geq \tau_{iI_j} \right).
\]
This function gives weight one to observations classified to part $I_a$ and zero to the others. Resulting to an average of $\lambda$ values for observations classified to part $I_a$. 

Although variations to functions $\omega$ and $\lambda$ have no limits, we present two alternatives to function $\lambda$ with a well founded background. \cite{aitchison1986statistical} introduces the main elements for the statistical analysis of samples defined in the Simplex space $\mathcal{S}^d$, i.e $\mathcal{S}^d = \{ (x_1,\dots, x_d) \;|\; x_i > 0 \text{ and } \sum_{i=1}^k x_i = 1 \}$. The central idea of this methodology is that only the ratios between variables are of interest. Here we take advantage that posterior probability vectors $\m\tau_{i\mathcal{P}_s}$ are defined in $\mathcal{S}^s$ to define two new functions $\lambda$. The first function is motivated by the Entropy concept introduced by \cite{baudry2010combining}, the second one is based on the definition given by \cite{longford2014}.


Following \cite{baudry2010combining} the closer the posterior probability vector $\m\tau_{i \mathcal{P}_{s-1}^{I_a\cup I_b}}$ is to the vector of the uniform distribution $\m\tau_s^0$, the higher is our preference to merge components $I_a$ and $I_b$ (the lower is the entropy after merging $I_a$ and $I_b$). To measure our preference for merging $I_a$ and $I_b$ we propose to calculate the similarity between the vector $\m\tau_{\left(I_a, I_b\right)} = (\frac{\tau_{i I_a}}{\tau_{i I_a} + \tau_{i I_b}}, \frac{\tau_{i I_b}}{\tau_{i I_a} + \tau_{i I_b}})$ and the vector $\m\tau_2^0=\left(\frac{1}{2}, \frac{1}{2}\right)$. That is, we restrict only on the probabilities associated to the components to be merged. According  \cite{frey2007}, we use an appropriate distance to define the similiarty measure. Because the probability vectors are elements of the Simplex space, we use the squared Aitchison distance \citep{palarea2012dealing} given by
\[
d_\mathcal{A}^2\left(\m\tau_{\left(I_a, I_b\right)}, \m\tau_2^0 \right) = \log^2 \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right).
\]

 to define the similarity function
\[
\lambda_{\text{dist}}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) := -d_\mathcal{A}\left(\m\tau_{\left(I_a, I_b\right)}, \m\tau_2^0 \right)^2 = -\log^2 \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right).
\]



Similarly to function $\lambda_{\Delta\text{Ent}}$, the function $\lambda_{\text{dist}}$  measures how similar is $\m\tau_{\left(I_a, I_b\right)}$ to $\m\tau_2^0$. However, the codomain of function $\lambda_{\Delta\text{Ent}}$ also implicitely depends of the posterior probabilities for the other parts different of those to be merged. Figure~\ref{symetric} shows these functions for a partition with three components $I_a$, $I_b$, and $I_c$. For different values of $\tau_{iI_c}$, the curves represent the effect of $\tau_{I_a}$ and $\tau_{I_b}$ into functions $\lambda$ . We see that both functions $\lambda$ take their maximum for $\tau_{iI_a}=\tau_{iI_b}=\frac{1-\tau_{i I_c}}{2}$. However, whereas the maximum value of function $\lambda_{\text{dist}}$ is zero regardless the value of $\tau_{iI_c}$ (Figure~\ref{symetric}(right)), the codomain of function $\lambda_{\text{Ent}}$ depends on $\tau_{iI_c}$ (Figure~\ref{symetric}(left)). \textcolor{red}{This fact suggests that the selection of the best components to be merged using $\lambda_{\text{Ent}}$ based on the corresponding $S$-value can be affected by the size of the clusters.} \textcolor{black!80!green}{In the particular case that $S_{\omega_{\text{cnst}}, \lambda_{\Delta \text{Ent}}}$, to decide the final number of clusters, \cite{baudry2010combining} noticed that the size could be affecting the decision. He proposed to divide the $S$-value by the number of individuals considered to be merged.}

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.8\textwidth]{figures/entr_dist.pdf} \\
 \end{tabular}
 \caption{Function $\lambda$ for posterior probability 
vector $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c} \right)$ with $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$: (left) $\lambda_{\text{Ent}_m}$; (right) $\lambda_{dist}$.} 
\label{symetric}
\end{center}
\end{figure}

Following the function $\lambda_{\text{DEMP}_m}$ introduced by \cite{longford2014} (Equation \label{lambda_dempM}), we can define another function $\lambda$ using log-ratios. We propose to measure the relative difference between $\tau_{iI_b}$ an $\tau_{iI_a}$ with the log-ratio
\[
\lambda_{\log}(\m\tau_{i \mathcal{P}_s},  I_a,  I_b) = \log \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right),
\]

\textcolor{red}{that increases when the probability $\tau_{iI_b}$ increases, and decreases when the probability $\tau_{iI_a}$ is increased.}

Figure~\ref{nonsymetric} shows the behaviour of functions $\lambda_{\text{DEMP}_m}$ and $\lambda_{\log}$ for posterior probability vectors $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c}\right)$ when $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$. \textcolor{red}{Note that the maximum value of both functions is not affected by the value $\tau_{iI_c}$. However, whereas the codomain of  function $\lambda_{\text{DEMP}_m}$ is th einterval $[0,1]$, the codomain of function $\lambda_{\log}$ is the Real space.} We can see that the highest values of function $\lambda_{\log}$ occurs for observations with high $\tau_{iI_b}$ relative to $\tau_{iI_a}$. Because the comparison is relative, observations not related with part $I_a$ i $I_b$ can play an important role to the final $S$-value. Therefore,it is especially important the selection of function $\omega(\m\tau_{i \mathcal{P}_s},  I_a,  I_b)$. A reasonable selection for function $\omega$ would be $\omega_{\text{prop}}$ or $\omega_{\text{dich}}$. In this context, function $\omega_{\text{cnst}}$ makes no sense.

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.8\textwidth]{figures/demp2_log.pdf} \\
 \end{tabular}
 \caption{Function $\lambda$ for posterior probability vector $\left(\tau_{iI_a}, \tau_{iI_b}, \tau_{iI_c} \right)$ with $\tau_{iI_c} \in \{0.2, 0.4, 0.6, 0.8\}$: (left) $\lambda_{\text{DEMP}_m}$; (right) $\lambda_{\log}$.} 
\label{nonsymetric}
\end{center}
\end{figure}

Due $\lambda_{\text{dist}}$ and $\lambda_{\log}$ use logratios they take into an account the geometric properties of the Simplex space \citep{aitchison2002simplicial}.  Working with log-ratios between the components of a posterior probability vector, it holds \emph{subcompositional coherence}  \citep{aitchison1986statistical}. This property guarantees that any statistical inference obtained using only partial information is coherent with results obtained using complete information. Formally, in our context sub-compositional coherence can be defined as:

\begin{defn}
Let $\mathcal{P}_1, \dots, \mathcal{P}_k$ be a hierarchical sequence of partitions obtained from posterior probability matrix $\text{\textbf{T}}_{\mathcal{P}_k}$ using a merging approach $M$. Let $I = \{j_1, \dots, j_s\}$ be an element (a part) of $\mathcal{P}_s$, for some $s$, $1\leq s \leq k$. A method M is \emph{sub-compositional coherent} if the hierarchy subsequence of partitions obtained  using only posterior probability vectors $\left\{ \left(\tau_{ij_1}, \dots, \tau_{ij_s} \right)\right\}_{1\leq i \leq n}$ is contained in original hierarchy.
\end{defn}

Another interesting feature when log-ratio approach is used is the \emph{scale invariance} property, formally
\[
S_{\omega, \lambda}( \m\tau_{\mathcal{P}_s},  I_a,  I_b) = S_{\omega, \lambda}(k\cdot \m\tau_{\mathcal{P}_s},  I_a,  I_b) \text{ for $k>0$.}
\] 

Scale invariance property suggests that the approach is not limited to posterior probability vectors but to any other kind of vector giving relative information between mixture components. In consequence, the methods considered here are suitable to be applied in more general scenarios such that those were instead of posterior probability vectors different weights are given to each part (e.g. weights in fuzzy clustering).

Finally, noting that when $\omega_{\text{prop}}$ and $\lambda_{\log}$ are considered, the $S$-value (Equation~\ref{unifying_equation}) results in
\[
S_{\omega, \lambda}( \m\tau_{\mathcal{P}_s},  I_a,  I_b) = \frac{\sum_{i=1}^n \tau_{iI_a}  \log \left(\frac{ \tau_{iI_b} }{ \tau_{iI_a} }\right)}{\sum_{i=1}^n \tau_{iI_a}}.
\]
For a fixed component $I_a$, the denominator is constant and we only need to maximise the numerator. The expression in the numerator has the flavour of the Kullback-Leibler divergence (in negative sign) comparing the distribution of classifying observations to $I_a$ against the distributions of classifying the same observations to $I_b$.

\section{Deciding the number of clusters}\label{number_clusters}

Given a finite mixture adjusted with BIC criteria, we have presented a generic approach to built a hierarchical sequence of partitions. One of the main difficulties is about deciding the final number of clusters. It is clear that for any partition $\{I_1, \dots, I_{s}\}$ the likelihood function of mixture $f = \pi_{I_1} f_{I_1} + \dots + \pi_{I_s} f_{I_s}$ is always the same. In other words, from a frequentist perspective, it is not possible to decide which of the different relation ways of modelling a cluster with different components is the best  \citep{hennig2010methods}. Therefore, we need to use heuristic methods to decide the final number of clusters.

\subsection{Using $S$-values}

The first option when deciding the number of clusters is the $S$-values. In the case of $\omega_{\text{cnst}}$ and $\lambda_{\Delta\text{Ent}}$, \cite{baudry2010combining} propose to visualise the $S$-values and apply the elbow rule. For $\omega_{\text{prop}}$ and $\lambda_{\text{DEMP}}$, \cite{hennig2010methods} propose to set an arbitrary threshold and stop merging when the $S$-value is lower than the fixed threshold. \textcolor{red}{Although there is not rule of thumb to define a method for the general merging criteria, for the particular proposals described in Section \ref{generic_merging} and \ref{logratio_section}, the $S$-values can be a useful tool for deciding the number of clusters. Indeed, from its definition as maximum wheigted average of $\lambda$ values, a $S$-value very \it{small} might suggest to stop the merging process}.

To better interpret the $S$-values we can normalise them to the interval $\left[0,1\right]$. Two options are feasible: to scale function $\lambda$ in the interval $\left[0,1\right]$ or to scale function $S_{\omega, \lambda}$. Indeed, because $S_{\omega, \lambda}$ is a weigthed mean of function $\lambda$, they have the same codomain. In consequence, any function $\phi$ that scales the function $\lambda$ also it scales the function $S_{\omega, \lambda}$. Let $S_{\omega, \phi \circ \lambda}$ and $\phi \circ S_{\omega, \lambda}$ be these two options for scaling to the interval $\left[0,1\right]$. Importantly, the first approach $S_{\omega, \phi \circ \lambda}$ is modifying function $\lambda$, and therefore, they can be considered as a new method by itself. It is worth mentioning that for $\phi_{\log}(x)=\frac{e^x}{1+e^x}$, the scaling $S_{\omega, \phi_{\log} \circ \lambda_{\log}}$ reduces to $S_{\omega, \lambda_{\text{DEMP}_m}}$, that is, the  function $\lambda_{\text{DEMP}_m}$ \citep{longford2014}, is a normalised version of the function $\lambda_{\log}$.

From its definition the functions $\lambda_{\Delta\text{Entr}}$, $\lambda_{\text{dist}}$ and 
$\lambda_{\log}$ are not scaled into the interval $\left[0,1\right]$. For scaling these functions we respectively propose  $\phi_{\Delta\text{Entr}}(x) = -x/{\log(\frac{1}{s})}$, $\phi_{\text{dist}}(x) = e^x$ and $\phi_{\log}(x) = \frac{e^x}{1+e^x}$. \textcolor{red}{Despite these functions are not the unique possibilities for the scaling, we selected them due its reasonable performance in our experiments. The corresponding scaled $S$-value functions are: 
  $\phi_{\Delta\text{Entr}} \circ S_{\omega, \lambda_{\Delta\text{Entr}}}$ or $S_{\omega, \phi_{\Delta\text{Entr}} \circ \lambda_{\Delta\text{Entr}}}$; $\phi_{\text{dist}} \circ S_{\omega, \lambda_{\text{dist}}}$ or $S_{\omega, \phi_{\text{dist}} \circ \lambda_{\text{dist}}}$; and $\phi_{\log} \circ S_{\omega, \lambda_{\log}}$ or $S_{\omega, \phi_{\log} \circ \lambda_{\log}}$. Once the $S$-values are normalised, one reasonable rule of thumb is to stop the merging process when these values are close to zero.}

\subsection{Using the clusters of the posterior probabilities}\label{coda_clusters}

When a sample consists of $s$ well separated clusters, if each cluster is modelled by a different probability distribution, the posterior probability vectors of the elements of each cluster should define a cluster close to a vertex in $\mathcal{S}^s$. That is, one value in these posterior probability vectors is approximately equal to one whereas the rest of values are close to zero. For example, in Figure~\ref{cluster_post} we have a sample generated following a mixture of four gaussian distributions. In Figure~\ref{cluster_post} (top-left) we cluster the sample in four clusters where each cluster is modelled by a single gaussian distribution. If we represent the posterior probability vectors on a quaternary diagram (top-right), we see that the posterior probabilty vectors for clusters associated to first and third components ($\tau_1$ and $\tau_3$) are located at the vertex of $\mathcal{S}^4$ and well separated from the rest. In contrast, we see that the posterior probability vectors for clusters associated to second and fourth components ($\tau_2$ and $\tau_4$) are not well separed between them. The situation changes if we separate the sample in three clusters (bottom-left), where one cluster (squared and green observations) is modelled by the mixture of two gaussian distributions. If we represent the posterior probability vectors in a ternary diagram (bottom-right), we see all posterior probabilities are forming a three separate clusters located at the vertices of $\mathcal{S}^3$. 


\begin{figure}[thbp]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
 % \includegraphics[width=0.7\textwidth]{figures/cluster_post.pdf} \\
\includegraphics[width=0.5\textwidth]{figures/ex_4clust.pdf}
\includegraphics[width=0.5\textwidth]{figures/ex_quaternaryb.pdf}\\
\includegraphics[width=0.5\textwidth]{figures/ex_3clust.pdf}
\includegraphics[width=0.5\textwidth]{figures/ex_ternary.pdf}
 \end{tabular}
 \caption{Top-Left: Sample following a mixture of four gaussian distributions. Each component forms a cluster. Top-Right: Posterior probability vectors of the sample represented in a quaternary diagram. Bottom-Left: Same sample were four components are modeling three clusters. Bottom-Right: Posterior probability vectors of the sample represented in a ternary diagram.}\label{cluster_post}
\end{center}
\end{figure}

In consequence, using the geometric structure of $\mathcal{S}^s$  we can introduce another approach to decide the number of clusters. \textcolor{red}{Indeed, using the geometric structure of $\mathcal{S}^s$ we can study how \emph{well} separated are the clusters. To this purpose, we study how \emph{well} the components $f_{I_1}, \dots, f_{I_s}$ are defining the clusters using distance based heuristics. To analyse the clusters formed by the posterior probability vectors we calculate the Calinski-Harabasz (G1) and the Goodman and Kruskal (G2) indices \citep{milligan1985}. For example, the indices $G1$ and $G2$ respectively take the values $350.28$ and $0.68$ for four clusters in the Figure~\ref{cluster_post} (top-right), whereas for three clusters the corresponding values are $759.49$ and $0.85$ (bottom-right).} The difference indicates that three clusters are defining a clustering better. We need to confirm that the defined clusters are located on the vertices of $\mathcal{S}^s$.

To decide how close are the clusters given by parts $I_1,\dots, I_s$ to the correspondent vertex of the Simplex we propose the following: for each pair of cluster $I_a$ and $I_b$ we calculate the average of the log-ratio between $\tau_{\cdot I_a}$ and $\tau_{\cdot I_b}$ for those observations classified to cluster $I_a$. Let $V_{a, b}$ be the minimum of the calculated averages. $\mathcal{V}$ can be interpreted as the minimum average distance of classifying observations classified into $I_a$ into $I_b$. Formally, $\mathcal{V}$ can be write as
\[
\min_{a=1}^s \min_{b=1;a\neq b}^s \frac{ \sum_{i=1}^n  \mathbbm{1}\left( \forall j\; \tau_{i I_{a}} \geq \tau_{iI_j} \right) \log(\frac{\tau_{iI_a}}{\tau_{iI_b}}) }{\sum_{i=1}^n\mathbbm{1}\left( \forall j\; \tau_{i I_{a}} \geq \tau_{iI_j} \right) }.
\]
A different interpretation of $V_{a,b}$ is by means of relation $\mathcal{V} = - \mathcal{S}_{\omega_\text{dich}, \lambda_{\log}}$. Suggesting that criteria $\mathcal{V}$ is measuring the higher confusion between two clusters. In this scenario, the higher $\mathcal{V}$  is  the closer the clusters are to the vertex. For example, the score is $1.64$ for four clusters in the Figure~\ref{cluster_post} (top-right) and $14.66$ for three clusters (bottom-right). Suggesting that after merging from four to three components the clusters are close to the vertex of $\mathcal{S}^3$.


\section{Merging components in a finite mixture: examples}\label{merging_examples_dist}

\subsection{Merging components in a mixture of Gaussian distributions}

Consider the bivariate Gaussian mixture of six components \citep{baudry2010combining}
\[
f= \sum_{j=1}^6 \pi_j \phi(\;\cdot\; ;  \m\mu_j, \m\Sigma_j)
\]
with the parameters shown in the Table~\ref{pars_table}. 

\begin{table}[htpb]
\centering
\begin{tabular}{rrrrrrr}
  \hline
$j$ & $\pi_j$ & $\mu_{j x_1}$ & $\mu_{j x_2}$ & $\sigma^2_{j x_1}$ & $\sigma^2_{j x_2}$ & $\rho_{j x_1 x_2}$ \\ 
  \hline
  1 &  $1/6$ &     0 &     0 &    50 &     5 &     0 \\ 
  2 &  $1/6$  &     0 &    40 &     5 &    50 &     0 \\ 
  3 &  $1/6$  &    40 &    40 &     5 &    50 &     0 \\ 
  4 &  $1/6$  &     0 &     0 &     5 &    50 &     0 \\ 
  5 &  $1/6$  &    40 &     0 &    50 &     5 &     0 \\ 
  6 &  $1/6$  &    40 &    40 &    50 &     5 &     0 \\ 
   \hline
\end{tabular}
\caption{Parameters defining a two dimensional Gaussian mixture with six components. The parameters $\m\mu_j$ and $\m\Sigma_j$ are expressed in terms of the univariate means $\mu_{j x_1}$, $\mu_{j x_2}$, the univariate variances $\sigma^2_{j x_1}$, $\sigma^2_{j x_2}$ and the correlation between $x_1$ and $x_2$, $\rho_{j x_1 x_2}$.}
\label{pars_table}
\end{table}


Let the parameters of $f$ be known. Figure~\ref{ex_mixture} shows the isodensity curves of the estimated \fmm for 
a random sample \textbf{X}. We are interested in clustering the sample \textbf{X}.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.7\textwidth]{figures/partition-example-mixture.pdf} \\
 \end{tabular}
 \caption{Density of Gaussian mixture of 6 components. Sample mean estimated of each component is represented by '+'.}\label{ex_mixture}
\end{center}
\end{figure}

The initial partition  $\mathcal{P}_6 = \{ \{1\},\{2\}, \{3\}, \{4\}, \{5\}, \{6\} \}$  by Equation~\ref{cluster_criteria} yields to a six clusters were each component is associated to one cluster. In Figure~\ref{ex_one_one} we have separate the observations in its respective cluster. In the plot we show isodensity curves for the density modelling each cluster. In this case each cluster is modelled with a Gaussian distribution.

\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=\textwidth]{figures/partition-example-part6.pdf} \\
 \end{tabular}
 \caption{Density of Gaussian mixture of 6 components where one cluster corresponds to one component}\label{ex_one_one}
\end{center}
\end{figure}

With the posterior probabilities $\text{\textbf{T}}_{\mathcal{P}_s}$ we calculate the $S$-values with $\omega_{\text{cnst}}$ and $\lambda_{\Delta\text{Ent}}$. We obtained the sequential hierarchical partition given by 
\begin{equation}
\begin{array}{r c l}
\mathcal{P}_6 &=& \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\},\\
\mathcal{P}_5 &=& \{\{1\},\{2\}, \{3, 6\},\{4\},\{5\} \},\\
\mathcal{P}_4 &=& \{\{1, 4\},\{2\}, \{3, 6\}, \{5\} \}, \\
\mathcal{P}_3 &=& \{\{1, 2, 4\}, \{3, 6\}, \{5\} \}, \\
\mathcal{P}_2 &=& \{\{1, 2 , 4, 5\}, \{3, 6\} \},  \\
\mathcal{P}_1 &=& \{\{1, 2, 3, 4, 5, 6\}\}.
\end{array}
\label{hier_ex}
\end{equation}

In the partition $\mathcal{P}_4 = \{\{1, 4\},\{2\}, \{3, 6\}, \{5\} \}$, the part $\{1, 4\}$ define a single cluster, as well as part \{3, 6\}. For this partition, using Equation~\ref{cluster_criteria} each observation $\m x_i$ is classified to one component. Figure~\ref{ex_two_one} shows the clustering with the isodensity curves defined by each component. In this case, clusters labelled $\{1,4\}$ and $\{3, 6\}$ are modelled by a mixture of two components. With partition $\mathcal{P}_4$ the clusters are modelled by $\fmm$s
\begin{itemize}
\item $f_{\{1,4\}} = \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_1, \m\Sigma_1) + \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_4, \m\Sigma_4)$, 
\item $f_{\{2\}} = \phi(\;\cdot\; ;  \m\mu_2, \m\Sigma_2)$, 
\item $f_{\{3,6\}} =  \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_3, \m\Sigma_3) + \frac{1}{2} \phi(\;\cdot\; ;  \m\mu_6, \m\Sigma_6)$ and
\item $f_{\{5\}} = \phi(\;\cdot\; ;  \m\mu_5, \m\Sigma_5)$.
\end{itemize}

\begin{figure}[h]
\begin{center}
\begin{tabular}{cc}
 %   6 toy mixture
  \includegraphics[width=0.65\textwidth]{figures/partition-example-part4.pdf} \\
 \end{tabular}
 \caption{Density of Gaussian mixture of 6 components where one cluster corresponds to one or more components}\label{ex_two_one}
\end{center}
\end{figure}

With $\omega_{prop}$ combined with $\lambda_{\text{DEMP}_m}$, $\lambda_{\text{dist}}$, $\lambda_{\log}$ and $\lambda_{\text{prop}}$ we obtain the same hierarchical partition (Equation \ref{hier_ex}). Using $\omega_{prop}$ and $\lambda_{\text{DEMP}}$ the sequential hierarchical obtained only differs from the previous one  in partition $\mathcal{P}_5$, where now $\mathcal{P}_5 = \{\{1, 4\},\{2\}, \{3\},\{5\},\{6\} \}$.

To decide the number of clusters we plot the $S$-values using different approaches (Figure~\ref{gaussian_Svalues}(a)). Using the Aitchison distance between posterior probabilities, Figure~\ref{gaussian_Svalues}(b) shows the average between and within clusters; and the Calinski-Harabasz (G1) and the Goodman and Kruskal (G2) indices \citep{milligan1985}. \textcolor{red}{no se si enlloc de la between i la within no seria millor monotoritzar algun index que indiques si hi ha grups que no estan a prop dun vertex. Per exemple el valor minim en el posteriori corresponent al cluster on shan classificat les observacions, el valor minim de la columna de la component vull dir}. Except the log-ratio approach before scaling, we see that all approaches after merging components to model four clusters or less the $S$-values are close to zero. This indicates that the clustering defined with four, three, or two clusters are well separated. Both G1 and G2 have a local maximum in four clusters, indicating that by those methods it is preferred to separate the data into four clusters. Combining the information obtained with the $S$-values and the indices G1 and G2 we separate our data into four clusters using partition $\mathcal{P}_4$ as shown in Figure~\ref{ex_two_one}.

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.85\textwidth]{figures/gaussian_Svalues.pdf} \\
   \includegraphics[width=0.65\textwidth]{figures/gaussian_statistics.pdf}
 \end{tabular}
 \caption{S-values obtained using the approaches presented by Baudry et al. and Hennig labeled Entropy and DEMP respectively.}\label{gaussian_Svalues}
\end{center}
\end{figure}

\subsection{Merging components in a mixture of multinomial distributions}\label{multinom_example}

Merging approaches presented in this article rely on the vector of posterior probabilities which can be calculated for any finite mixture model. Therefore, the merging generic approach introduced in Section~\ref{generic_merging} can be used for any family of \fmm, for example a finite mixture of multinomial distribution.

Pigs dataset can be obtained from package \emph{zCompositions} R package \citep{palarea2015zcompositions}. The dataset contains count data of behavioural observations of a group of 29 sows. In different moments the pigs were recorded during five minutes and its current activity was registered. For each pig six locations were considered: straw bed (BED), half in the straw bed (HALF.BED), dunging passage (PASSAGE), half in the dunging passage (HALF.PASS), feeder (FEEDER) and half in the feeder (HALF.FEED).


\begin{table}[t]
\centering
\begin{tabular}{rrrrrrrr}
  \hline
 Comp.& $\pi_j$ & $\theta_{j1}$ & $\theta_{j2}$ & $\theta_{j3}$ & $\theta_{j4}$ & $\theta_{j5}$ & $\theta_{j6}$ \\ 
  \hline
  1 & 0.0695 & 0.0103 & 0.0000 & 0.2874 & 0.0103 & 0.6867 & 0.0052 \\ 
  2 & 0.1710 & 0.0144 & 0.0000 & 0.0717 & 0.0020 & 0.9057 & 0.0062 \\ 
  3 & 0.0699 & 0.0817 & 0.0102 & 0.1390 & 0.0000 & 0.7538 & 0.0154 \\ 
  4 & 0.1724 & 0.1567 & 0.0082 & 0.7835 & 0.0021 & 0.0454 & 0.0041 \\ 
  5 & 0.0345 & 0.9485 & 0.0000 & 0.0309 & 0.0000 & 0.0206 & 0.0000 \\ 
  6 & 0.4828 & 0.7408 & 0.0147 & 0.1694 & 0.0074 & 0.0626 & 0.0052 \\  
   \hline
\end{tabular}
\caption{Parameters of a finite mixture of multinomial distributions adjusted to the Pigs data set. For component $j$, the mixing proportions are denoted by $\pi_j$ and the multinomial probabilities by $\left(\theta_{j1}, \dots, \theta_{j6}\right)$.}\label{multinomial_pars}
\end{table}

We used \emph{mixtools} \citep{benaglia2009mixtools} to fit a multinomial mixture. Six components were identified as optimum according to BIC criterion. In Table~\ref{multinomial_pars} the parameters of each components are shown. Using this parameters we can compute the posterior probability matrix $\text{\textbf{T}}_{\mathcal{P}_6}$. Each observation is classified into one cluster following Equation~\ref{map_criteria} or Equation~\ref{cluster_criteria} with partition $\mathcal{P}_6 = \{\{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}\}$. In Figure~\ref{multinomial_mixture} we can see the bar plot for observations classified into the same cluster.

\begin{figure}[!t]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.95\textwidth]{figures/multinomial_mixt_all.pdf} \\
 \end{tabular}
 \caption{Components after adjusting a six mixture of multinomial distributions. For each cluster, the relative amount of time seen in each location is shown.}\label{multinomial_mixture}
\end{center}
\end{figure}

Using $\omega_{\text{dich}}$ or  $\omega_{\text{prop}}$ with $\lambda_{\text{dist}}$ or $\lambda_{\log}$ we obtained the hierarchical structure partition given by
\begin{equation}
\begin{array}{r c l}
 \mathcal{P}_6&=& \;\; \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}, \\
 \mathcal{P}_5&=& \;\; \{\{1\},\{2\},\{3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_4&=& \;\; \{\{1,2\},\{3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_3&=& \;\; \{\{1,2,3\},\{4\},\{5,6\}\}, \\ 
 \mathcal{P}_2&=& \;\; \{\{1,2,3\},\{4,5,6\}\}, \\ 
 \mathcal{P}_1&=& \{\{1,2,3,4,5,6\}\}.
\end{array}
\label{hier_ex_multinomial}
\end{equation}

Other criteria like $S_{\omega_{\text{csnt}},\lambda_{\Delta\text{Ent}}}$,  $S_{\omega_{\text{prop}},\lambda_{\text{DEMP}}}$,  and $S_{\omega_{\text{prop}},\lambda_{\text{prop}}}$ differed only in partitions $\mathcal{P}_5$ and $\mathcal{P}_4$.  That is, these methods preferred to merge first the part $\{1,2,3\}$ obtaining partitions $\mathcal{P}_5 = \{\{1\},\{2, 3\},\{4\},\{5\} ,\{6\}\}$ and $\mathcal{P}_4 = \{\{1,2,3\},\{4\},\{5\},\{6\}\}$.

\begin{figure}[thpb]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.85\textwidth]{figures/multinomial_Svalues_all.pdf} \\
  \includegraphics[width=0.65\textwidth]{figures/multinomial_statistics.pdf} 
 \end{tabular}
 \caption{Squared average distance between clusters (top-left), the squared average distance within clusters (top-right), the Calinski-Harabasz index (bottom-left) and the Goodman and Kruskal's Gamma coefficient (bottom-right) applied to the posterior probabilities obtained after adjusting a finite mixture of multinomial distributions to the Pigs data set.}\label{multinomial_statistics}
\end{center}
\end{figure}

\begin{figure}[htpb]
\begin{center}
\begin{tabular}{cc}
  \includegraphics[width=0.95\textwidth]{figures/multinomial_clust3_all.pdf} \\
 \end{tabular}
 \caption{Pig dataset components after clustering the six mixture components in a 3-\fmm. For each cluster, the relative amount of time seen in each location is shown.}\label{multinomial_clust3}
\end{center}
\end{figure}

Figure~\ref{multinomial_statistics} shows the $S$-values given by different approaches (upper plot). We also plot the between and within averaged squared distances and the G1 and G2 index (lower plot). Except for the log-ratio approach (before and after scaling) and the squared distance, the $S$-values are close to zero after merging the components into four clusters. The squared distance and the log-ratio approach have an $S$-value close to zero after merging the components into five clusters. Depending on the method, when the clustering is modelled by five (four) clusters or less, the clusters are well separated. Inspecting the plot with the G1 and G2 indices, it indicates that the best choice is two clusters for $G1$ and three clusters for $G2$. Figure~\ref{multinomial_clust3} shows the bar plot for those observations assigned to each of the three clusters. The first cluster contains components one, two and four, all of them represented by sows with a high amount of time feeding. The second cluster contains components three and five, this cluster is characterized by sows with high amount of time in bed. Finally, the third cluster is form with a single component six which have a higher amount of time in the passage.



\section{Final remarks}\label{remarks}

\textcolor{red}{NO HO MIRARE FINS QUE LA RESTA ESTIGUI FET} When \fmm is used in clustering the question \textit{``is a cluster determined by a unique component?''} emerges. Different authors have proposed scenarios where it seems reasonable to argue that a cluster can be better modelled by more than one single component, or equivalently, modelled by a \fmm itself. In this scenarios, the approaches proposed in this article can be of interest.

In literature different approaches have been considered to merge the components of a \fmm. Some of them are specific to a particular type of mixture (methods related to Gaussian mixtures) and some others are independent of the type of mixture (to cite generic approaches). 

In this article we propose a generic approach that includes different criterias proposed earlier in the literature. The proposed generic approach relies only on the posterior probability vectors, and therefore, it is independent from the family of mixture. All the methods described in this article can be applied with any \fmm. We have illustrated the approach with a numerical sample and with categorical sample. 

The log-ratio approaches introduced in this article (Section~\ref{logratio_section}) and the indices defined on the posterior probabilities (Section~\ref{coda_clusters}) use the geometric structure of the Simplex space. Working with the posterior probabilities as an elements of the Simplex space permits to extend the results obtained here in other areas. For example, it would be interesting to analyse if in fuzzy-clustering the role played by the weights is equivalent to the role played by the posterior probabilities. 

Finally, to decide final number of clusters we have proposed to combine the information given by the scaled $S$-values and to analyse some heuristic indices using Aitchison distances between posterior probabilities and  the Calinski-Harabasz index or  the Goodman and Kruskal index. To the best of our knowledge it is the first time that these two options has been proposed to study the cluster structure. This new approach allows to study the structure of numerical and categorical data sets based on the considered model.


%%% BIBLIOGRAPHY

%\bibliographystyle{apalike}
\begin{thebibliography}{}

\bibitem[Aitchison, 1986]{aitchison1986statistical}
Aitchison, J. (1986).
\newblock {\em {The Statistical Analysis of Compositional Data}}.
\newblock Monographs on Statistics and Applied Probability. Chapman \& Hall Ltd., London (UK). Reprinted (2003) with additional material by The Blackburn Press, Caldwell, NJ.

\bibitem[Aitchison, 2002]{aitchison2002simplicial}
Aitchison, J. (2002).
\newblock {\em {Simplicial inference}}.
\newblock {\em Algebraic Methods in Statistics anb Probability}, 287: 1--22.

\bibitem[Baudry et~al., 2010]{baudry2010combining}
Baudry, J.P., Raftery, A.~E., Celeux, G., Lo, K., and Gottardo, R. (2010).
\newblock {Combining Mixture Components for Clustering}.

\bibitem[Benaglia et~al., 2009]{benaglia2009mixtools}
Benaglia, T., Chauveau, D., Hunter, D.R. and Youn, R. (2009).
\newblock {mixtools: An R Package for Analyzing Finite Mixture Models}.
\newblock {\em Journal of Statistical Software}, 32(6), 1--29.

\bibitem[Fraley and Raftery, 1998]{fraley1998how}
Fraley, C. and Raftery, A. E. (1998).
\newblock {How many clusters? Answers via model-based cluster analysis}.
\newblock {\em The computer Journal}, 41:578--588.

\bibitem[Fraley and Raftery, 2002]{fraley2002model}
Fraley, C. and Raftery, A. E. (2002).
\newblock {Model-Based Clustering, Discriminant Analysis, and Density Estimation}.
\newblock {\em Journal of the American Statistical Association}, 97(458):611--631.

\bibitem[Frey and Dueck, 2007]{frey2007}
Frey, B.J. and Dueck, D. (2007).
\newblock {Clustering by passing messages between data points}.
\newblock {\em Science}, 315:972--976.

\bibitem[Hennig, 2010]{hennig2010methods}
Hennig, C. (2010).
\newblock {Methods for merging Gaussian mixture components}.
\newblock {\em Advances in Data Analysis and Classification}, 4(1):3--34.

\bibitem[Keribin, 1998]{keribin1998consistent}
Keribin, C. (1998).
\newblock {Consistent estimate of the order of mixture models}.
\newblock {\em Comptes Rendues de l’Academie des Sciences, Série I-Mathématiques}, 326:243--248.


\bibitem[Keribin, 2000]{keribin2000consistent}
Keribin, C. (2000).
\newblock {Consistent Estimation of the Order of Mixture Models}.
\newblock {\em Sankhy\={a}: The Indian Journal of Statistics, Series A}, 62(1):49--66.

\bibitem[Lee and Cho, 2004]{lee2004combining}
Lee, H.J. and Cho, S. (2004).
\newblock {Combining Gaussian Mixture Models}.
\newblock In Yang, Z., Yin, H., and Everson, R., editors, {\em Intelligent Data
  Engineering and Automated Learning – IDEAL 2004 SE - 98}, volume 3177 of
  {\em Lecture Notes in Computer Science}, pages 666--671. Springer Berlin
  Heidelberg.

\bibitem[Longford and Bartosova, 2014]{longford2014}
Longford, N.~T. and Bartosova, J. (2014).
\newblock {A confusion index for measuring separation and clustering}.
\newblock {\em Statistical Modelling}, 14(3):229--255.

\bibitem[McLachlan, 2014]{mclachlan2014components}
McLachlan, G. J. and Rathnayake S. (2014).
\newblock {On the number of components in a Gaussian mixture model}.
\newblock {\em Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},  4:341--355.

\bibitem[Melnykov, 2013]{melnykov2013distribution}
Melnykov, V. (2013).
\newblock {On the Distribution of Posterior Probabilities in Finite Mixture Models with Application in Clustering}.
\newblock {\em Journal of Multivariate Analysis}, 122:175--189.

\bibitem[Milligan, 1985]{milligan1985}
Milligan, G. W. and Cooper, M. C. (1985) 
\newblock {An examination of procedures for determining the number of clusters}. 
\newblock {\em Psychometrika}, 50:159-179.

\bibitem[Palarea-Albaladejo et~al., 2012]{palarea2012dealing}
Palarea-Albaladejo, J., Martín-Fernández, J.A. and Soto, J.A. (2012).
\newblock {Dealing with Distances and Transformations for Fuzzy C-Means Clustering of Compositional Data}.
\newblock {Journal of Classification}, 29(2):144--169.


\bibitem[Palarea-Albaladejo and Martín-Fernández, 2015]{palarea2015zcompositions}
Palarea-Albaladejo J. and Martin-Fernandez J.A. (2015).
\newblock {zCompositions - R packages for multivariate imputation of nondetecteds and zeros in compositional data sets}.
\newblock{\em Chemometrics and Intelligent Laboratory Systems}, 143:85--96.

\bibitem[Pastore and Tonellato, 2013]{pastore2013merging}
Pastore, A. and Tonellato, S.~F. (2013).
\newblock {A Merging Algorithm for Gaussian Mixture Components}.
\newblock {\em SSRN Electronic Journal}, (04).

\bibitem[Punzo, 2014]{punzo2014flexible}
Punzo, A. (2009).
\newblock {Flexible mixture modelling with the polynomial Gaussian cluster-weighted model}.
\newblock {\em Statistical Modelling}, 14(3):257--291.

\end{thebibliography}

%%%%%%%%%%% END SPACING
%\end{spacing}

\end{document}
